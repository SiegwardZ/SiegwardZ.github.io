---
layout: post
title: Notes
categories: [Technology,Notes]
tags: [technology,notes]
description: A technical notes for recording encountered issues and techniques.
comments: false
math: true
date: 2024-09-30 21:00 +0800
---
## AI Models
### CLIP Encoder
Huggingface的CLIP Model类中的部分代码如下
```python
text_model = CLIPTextModel._from_config(text_config)
self.text_model = text_model.text_model

vision_model = CLIPVisionModel._from_config(vision_config)
self.vision_model = vision_model.vision_model

self.visual_projection = nn.Linear(self.vision_embed_dim, self.projection_dim, bias=False)
self.text_projection = nn.Linear(self.text_embed_dim, self.projection_dim, bias=False)
```
实际encode的主要过程如下(vision 同理)
```python
text_outputs = self.text_model(
          input_ids=input_ids,
          attention_mask=attention_mask,
          position_ids=position_ids,
          output_attentions=output_attentions,
          output_hidden_states=output_hidden_states,
          return_dict=return_dict,
      )

pooled_output = text_outputs[1]
text_features = self.text_projection(pooled_output)
```
clip model相当于先用各自的encoder encode出一个embedding之后，做了一次pooling，最后再用linear projector投影到相同的特征空间中。这里的pooling操作取的是eos_token的hiddens_state。

比如```openai/clip-vit-large-patch14-336```的model，

其text经过text model输出的是一个(1,200,768)的tensor，即(batch,seq_len,hidden_size)，pooled之后得到(1,768)的tensor，最后经过projector投影成(1,1024)的feature embedding。

而image经过imageprocessor先变成(1,3,336,336)的tensor，即(batch, channel, resolution_width, resolution_height)，经过vision model输出的是一个(1,577,1024)的tensor，pooled之后得到(1,1024)的tensor，最后经过projector投影成(1,1024)的feature embedding。

注意这里之所是(1,577,1024)的shape是因为该clip model划分的patch数为14，宽高都是336，那么划分出的patch总个数为

$$
\frac{336}{14} \times \frac{336}{14} = 24 \times 24 = 576
$$ 

再加上开头的CLS token共577个token。pooling时也是使用CLS token作为pooling token。

### Cross-Attention
```python
if is_cross_attention:
    key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))
    value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))
    attention_mask = encoder_attention_mask
elif past_key_value is not None:
    key_layer = self.transpose_for_scores(self.key(hidden_states))
    value_layer = self.transpose_for_scores(self.value(hidden_states))
    key_layer = torch.cat([past_key_value[0], key_layer], dim=2)
    value_layer = torch.cat([past_key_value[1], value_layer], dim=2)
else:
    key_layer = self.transpose_for_scores(self.key(hidden_states))
    value_layer = self.transpose_for_scores(self.value(hidden_states))
```
如果是cross attention的话，在decoder层传入的encoder_hidden_states参数是用来在cross attention中做为key和value的。query仍为input_ids。

### tokenizer add special token
可以使用 `add_special_tokens` 方法为现有的 `Tokenizer` 添加特殊标记。这个方法适用于已经加载好的 `Tokenizer` 对象。

```python
from transformers import AutoTokenizer

# 加载 tokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# 定义特殊标记
special_tokens_dict = {'bos_token': '[BOS]', 'eos_token': '[EOS]', 'additional_special_tokens': ['[SPECIAL1]', '[SPECIAL2]']}

# 添加特殊标记
num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)

# 更新模型的 embedding 大小（如果需要）
model.resize_token_embeddings(len(tokenizer))

print(f"添加了 {num_added_toks} 个特殊标记")
```



## Pytorch 
### gradient 计算
如果已知梯度公式但没法表示原函数，有如下几种方法计算梯度：
1. 根据梯度反推出另一个原函数，用那个原函数作为loss function计算梯度，常见操作如将 $ \nabla f(x) $ 替换成 $ f(x) \nabla log f(x) $
2. 根据梯度公式，通过冻结部分参数的方式实现梯度计算，即把不需要计算梯度的部分当作常数，将它们放在no_grad下计算

### 查看module的gradient
查看module参数的梯度:
```python
for name, param in model.named_parameters():
    if param.grad is not None:
        print(f"grad of param {name}: \n {param.grad}")
    else:
        print(f"param {name} has no grad")
```
### 两次调用loss backward后报错 计算图不能使用两次
看看是不是调用`optimizer.zero_grad()`后复用了前一个loss的计算图
每次清空梯度后需要从头开始计算下一个loss

### expand方法
*from GPT❤*

`.expand()` 函数在 PyTorch 中用于扩展张量的维度，使其在指定维度上重复，而不实际分配新的内存。这样可以在不增加内存使用的情况下，改变张量的形状。使用 `.expand()` 时，原始张量的内容不会被复制，而是通过广播机制（broadcasting）来实现。

主要特点包括：

1. 维度扩展: `.expand()` 可以将张量在指定的维度上扩展到更大的大小。例如，如果一个张量的形状是 `(1, 3)`，使用 `.expand(4, 3)` 将返回一个形状为 `(4, 3)` 的张量。

2. 不复制数据: 扩展后的张量与原始张量共享内存，因此不会增加内存占用。这对于大型张量的操作尤为重要。

3. 广播机制: `.expand()` 的使用依赖于广播机制，这意味着你可以将较小的张量与较大的张量相加或进行其他运算，而不会引发尺寸不匹配的错误。

假设你有一个形状为 `(1, 3)` 的张量：
```python
import torch

a = torch.tensor([[1, 2, 3]])  # shape: (1, 3)
b = a.expand(4, 3)              # shape: (4, 3)

print(b)
```
输出：
```
tensor([[1, 2, 3],
        [1, 2, 3],
        [1, 2, 3],
        [1, 2, 3]])
```
虽然 `b` 看起来包含了四个相同的行，但它实际上只是通过广播引用了 `a` 的数据，没有进行实际的内存复制。

nn.Parameter也可以使用expand方法实现对参数的维度扩展

### 查找指定元素位置
```python
import torch

tensor = torch.tensor([
    [[1, 2, 3], [4, 5, 6]],
    [[7, 8, 9], [3, 2, 1]]
])

value = 3
# method 1
indices = torch.where(tensor == value)
# method 2
indices = (tensor == value).nonzero(as_tuple=True)

print("Indices of value 3:", indices)
```
结果返回的是符合条件的索引在各个维度的具体位置
```text
Indices of value 3: (tensor([0, 1]), tensor([0, 1]), tensor([2, 0]))
```
如果需要每个符合条件的位置组合成元组列表，可以将结果转化为 zip 形式：
```python
indices = list(zip(*torch.where(tensor == value)))
print("List of indices:", indices)
```
这样就能得到形如`[(0,0,2),(1,1,0)]`的结果

## Markdown 
### 并排显示表格
1. html

```html
<p align="center">
    <img src="" alt="Image 1" width="45%" />
    <img src="" alt="Image 2" width="45%" />
</p>
```

2. table
```markdown
| ![Image 1](image1.png) | ![Image 2](image2.png) |
|------------------------|------------------------|
```

### 展示html代码块
注意\`\`\`前如果是html，需要空一行

## Python
### remove dirs
使用```os.removedirs(path)```会遇到检测到目录非空而无法删除的问题，可换成```shutil.rmtree(path)```，注意需要```import shutil```

### vars转dict
使用```vars()```可以转换特定对象为dict格式，等价于调用```object.__dict__()```，如将argparse返回的namespace转换成config dict

### 调用摄像头
```python
import numpy as np
import cv2

cap = cv2.VideoCapture(0)
w = int(cap.get(3))  # 获取视频的width
h = int(cap.get(4))  # 获取视频的height
fourcc = cv2.VideoWriter_fourcc(*'XVID')
out = cv2.VideoWriter('output.avi',fourcc, 30.0, (w, h))

while(cap.isOpened()):
    ret, frame = cap.read()
    if ret==True:
        frame = cv2.flip(frame,1)
        out.write(frame) # 保存视频
        cv2.imshow('frame', frame)
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break
    else:
        break

cap.release()
out.release()
cv2.destroyAllWindows()
```
ref:[https://blog.csdn.net/qq_45779334/article/details/114676905](https://blog.csdn.net/qq_45779334/article/details/114676905)

### pdb调用栈
w或where查看当前调用栈
up和down在调用栈帧上下移动

### python dict 合并
方法 1：使用 `update()` 方法
将 `dict_b` 的键值对添加到 `dict_a` 中（此操作会直接修改 `dict_a`）。

```python
dict_a.update(dict_b)
```

此方法会在 `dict_a` 中更新或添加 `dict_b` 中的键值对，**原地修改 `dict_a`**，如果有相同的键，`dict_a` 中的值会被 `dict_b` 的值覆盖。

方法 2：使用解包运算符 `**`
可以通过创建一个新的字典，将 `dict_a` 和 `dict_b` 解包后合并在一起。

```python
merged_dict = {**dict_a, **dict_b}
```

这种方式不会修改原字典，而是创建了一个新的字典 `merged_dict`。如果存在相同的键，`dict_b` 中的值会覆盖 `dict_a` 的值。

方法 3：使用 `|` 运算符（Python 3.9+）
Python 3.9 及以上版本支持字典的并集运算符 `|`，可以直接用它合并两个字典。

```python
merged_dict = dict_a | dict_b
```

这种方式同样会生成一个新的字典 `merged_dict`，不会修改原字典，并且如果存在相同的键，`dict_b` 中的值会覆盖 `dict_a` 的值。

方法 4：使用字典生成式
可以通过字典生成式来合并两个字典，也可以实现更复杂的合并逻辑。

```python
merged_dict = {k: v for d in (dict_a, dict_b) for k, v in d.items()}
```

这个方式也会创建一个新的字典 `merged_dict`，并且 `dict_b` 中的值会覆盖 `dict_a` 中的值。

### 不重复随机数
如下例，生成10个1-100范围内不同的随机整数
```python
random_numbers = random.sample(range(1,101),10)
```

### coding 编码
在脚本开始加上`# -*- coding: UTF-8 -*-`可以显式声明脚本编码方式，是python的coding声明，一般python2默认使用ASCII，python3默认使用UTF-8

### @装饰器
在 Python 中，函数前的 @ 符号用于“装饰器”（decorator）。装饰器是一种特殊的函数，它用于修饰或增强其他函数或方法的功能。装饰器通常用于在不修改原函数代码的情况下，增加一些预定义的操作或逻辑。

常见装饰器如
+ @staticmethod：定义静态方法。
+ @classmethod：定义类方法。
+ @property：将方法转换为属性。

### 方法签名
方法签名（Method Signature）是指一个方法（或函数）在程序中的定义部分，包括方法的名称、参数列表（包括参数的名称、顺序、类型信息等），以及可能的返回类型。方法签名用于明确描述方法的接口，即该方法可以接受哪些输入，返回什么类型的输出。
可以使用 `inspect.signature()` 获得某个函数或方法的签名信息。

例如
```python
import inspect

def add(a: int, b: int) -> int:
    return a + b

signature = inspect.signature(add)
print(signature)
```
输出为
```python
(a: int, b: int) -> int
```

## Windows
### 双网卡路由配置
以管理员方式打开powershell

+ `route print`可以查路由表
+ `route add [-p] [target ip] mask [netmask] [gateway] if [interface]` 添加一条路由规则，其中
    + `-p`设为永久路由
    + interface 可以通过print的信息找到
+ `route delete [target ip]`可以删除一条路由规则
```shell
# 双网卡情况下可以一个设为外网，一个设为内网，例
route add -p 0.0.0.0 mask 0.0.0.0 192.168.1.1 if 12
route add -p 10.128.0.0 mask 255.128.0.0 10.249.8.1 if 2
```

## Linux
### Shell同时输出到文件和控制台
```bash
<your program> | tee <your file path>
```
### 查看tcp端口占用
```bash
lsof -wni tcp:4000
```
### 递归删除当前目录下所有特定文件
*from GPT ❤*
```bash
find . -type f -name "<filename>" -exec rm -f {} +
```

命令解释：

- `find .`：在当前目录 (`.`) 及其子目录中查找文件。
- `-type f`：仅查找文件（不包含目录）。
- `-name "<filename>"`：仅查找文件名为 "\<filename\>" 的文件。
- `-exec rm -f {} +`：对查找到的每个文件执行删除命令 `rm -f`。`{}` 代表查找到的文件，`+` 表示批量处理。

### shebang
脚本开头的`#!/bin/bash`是一个哈希邦，用来指定脚本的解释器，确保无论在什么环境执行，都会使用该解释器


## Docker
### 设置端口绑定
以绑定container 22端口至主机50001端口, 以实现ssh访问container为例
1. 创建时绑定(-p)
```bash
docker run -it <image> -p 50001:22
```
2. 创建后绑定
- 完全关闭docker(windows包括整个docker desktop)
- 找到container文件所在位置
    - 修改config_v2.json
        - 在config中加上``"ExposedPorts": {"22/tcp": {}}`
        - 在networksettings中加上`"Ports": {"22/tcp": [{"HostIp": "0.0.0.0","HostPort": "50001"}]}`
    - 修改hostconfig.json文件, 修改PortBinds为`{"22/tcp":[{"HostIp":"","HostPort":"50001"}]}`
- 保存修改,重启docker即可


### windows docker文件位置
在`\\wsl$`地址内
container文件位于`\\wsl.localhost\docker-desktop-data\data\docker\containers`


## Development Tools
### vscode
+ python
+ markdown all in one
+ comment translator
+ background
+ Bookmarks
+ Dev Containers
+ CMake
+ Github Copilot
+ C/C++
+ Chinese
### system
+ cpu-z
+ CrystalDiskMark
+ vscode
+ git
+ IDM
+ docker 
+ wireshark
