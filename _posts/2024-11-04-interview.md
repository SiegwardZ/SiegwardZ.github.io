---
layout: post
title: Notes for NLP interview
date: 2024-11-04 19:52 +0800
categories: [Study, NLP]
comment: false
math: true
tags: [notes,ai,nlp,interview]
---
##  Transformers
### Transformers ç»“æ„
![transformer-architecture](/images/notes/transformer-architecture.png){:w="400" h="700"}
![transformer-attention-formulation](/images/notes/transformer-attention-formulation.png)


### Multi-head Attention æºç è§£æ
```python
# from transformers.models.modeling_bart
class BartAttention(nn.Module):
    """Multi-headed attention from 'Attention Is All You Need' paper"""
    def __init__(
        self,
        embed_dim: int,
        num_heads: int,
        dropout: float = 0.0,
        is_decoder: bool = False,
        bias: bool = True,
        is_causal: bool = False,
        config: Optional[BartConfig] = None,
    ):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.dropout = dropout
        self.head_dim = embed_dim // num_heads
        self.config = config

        if (self.head_dim * num_heads) != self.embed_dim:
            raise ValueError(
                f"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim}"
                f" and `num_heads`: {num_heads})."
            )
        self.scaling = self.head_dim**-0.5
        self.is_decoder = is_decoder
        self.is_causal = is_causal

        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)
        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)
        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)
        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)
```
init å‡½æ•°

```python
    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):
        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous() 
        # ps: tensorçš„ç»´åº¦æ˜¯(bsz,seq_len,hidden_sz)ï¼Œhidden_sz = num_heads*head_dim, ä½¿ç”¨viewå°†tensorè½¬ä¸º(bsz,seq_len,num_heads,head_dim)ï¼Œtransposeè°ƒæ¢ç¬¬ä¸€ç»´å’Œç¬¬äºŒç»´ï¼Œå˜ä¸º(bsz, num_heads, seq_len, head_dim)ï¼Œä¾¿äºåç»­å¤„ç†ä¸åŒå¤´çš„æ³¨æ„åŠ›
```
+ tensorçš„ç»´åº¦æ˜¯(bsz,seq_len,hidden_sz)ï¼Œhidden_sz = num_heads*head_dim, ä½¿ç”¨viewå°†tensorè½¬ä¸º(bsz,seq_len,num_heads,head_dim)ï¼Œtransposeè°ƒæ¢ç¬¬ä¸€ç»´å’Œç¬¬äºŒç»´ï¼Œå˜ä¸º(bsz, num_heads, seq_len, head_dim)ï¼Œä¾¿äºåç»­å¤„ç†ä¸åŒå¤´çš„æ³¨æ„åŠ›

```python
    def forward(
        self,
        hidden_states: torch.Tensor,
        key_value_states: Optional[torch.Tensor] = None,
        # encoder_outputs å°±æ˜¯encoder_hidden_states (batch_sz,encoder_seq_len,hidden_sz)
        past_key_value: Optional[Tuple[torch.Tensor]] = None, 
        # decoderä½¿ç”¨cross-attentionï¼Œéœ€è¦encoder_outputsä½œä¸ºkeyå’Œvalueã€‚decoderåœ¨ç”Ÿæˆæ—¶éœ€è¦é‡å¤è°ƒç”¨forwardï¼Œè€Œæ¯æ¬¡forwardä¸­keyå’Œvalueéƒ½æ˜¯ä¸€æ ·çš„ï¼Œæ‰€ä»¥åªéœ€è¦åœ¨ç¬¬ä¸€æ¬¡forwardæ—¶å°†encoder_outputsæ ¹æ®k_projå’Œv_projä»¥åŠå¤šå¤´åˆ†å‰²è½¬å˜ä¸ºkeyå’Œvalueï¼Œç„¶åå°†å…¶ä¿å­˜åœ¨past_key_valueä¸­ï¼Œå°±å¯é‡å¤ä½¿ç”¨ï¼Œé¿å…äº†æ¯æ¬¡forwardéƒ½é‡å¤è°ƒç”¨k_projå’Œv_projè¿›è¡Œè®¡ç®—ã€‚åœ¨ç”Ÿæˆæ—¶æœ‰æ•ˆæå‡æ¨ç†æ€§èƒ½ã€‚
        attention_mask: Optional[torch.Tensor] = None,
        layer_head_mask: Optional[torch.Tensor] = None,
        output_attentions: bool = False,
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
        """Input shape: Batch x Time x Channel"""

        # if key_value_states are provided this layer is used as a cross-attention layer
        # for the decoder
        is_cross_attention = key_value_states is not None 
        # ps: Encoder is self-attention, decoder requires cross-attention

        bsz, tgt_len, _ = hidden_states.size() # ps: (batch_sz,decoder_seq_len,hidden_size)

        # get query proj
        query_states = self.q_proj(hidden_states) * self.scaling  # ps: Q = W_q * hidden_states:  (batch_sz,decoder_seq_len,hidden_size) 
```
+ key_value_stateså°±æ˜¯encoder_hidden_states (batch_sz,seq_len,hidden_sz)
+ decoderä½¿ç”¨cross-attentionï¼Œéœ€è¦encoder_outputsä½œä¸ºkeyå’Œvalueã€‚decoderåœ¨ç”Ÿæˆæ—¶éœ€è¦é‡å¤è°ƒç”¨forwardï¼Œè€Œæ¯æ¬¡forwardä¸­keyå’Œvalueéƒ½æ˜¯ä¸€æ ·çš„ï¼Œæ‰€ä»¥åªéœ€è¦åœ¨ç¬¬ä¸€æ¬¡forwardæ—¶å°†encoder_outputsæ ¹æ®k_projå’Œv_projä»¥åŠå¤šå¤´åˆ†å‰²è½¬å˜ä¸ºkeyå’Œvalueï¼Œç„¶åå°†å…¶ä¿å­˜åœ¨past_key_valueä¸­ï¼Œå°±å¯é‡å¤ä½¿ç”¨ï¼Œé¿å…äº†æ¯æ¬¡forwardéƒ½é‡å¤è°ƒç”¨k_projå’Œv_projè¿›è¡Œè®¡ç®—ã€‚åœ¨ç”Ÿæˆæ—¶æœ‰æ•ˆæå‡æ¨ç†æ€§èƒ½ã€‚

```python
        # get key, value proj
        # `past_key_value[0].shape[2] == key_value_states.shape[1]`
        # is checking that the `sequence_length` of the `past_key_value` is the same as
        # the provided `key_value_states` to support prefix tuning

        if (
            is_cross_attention
            and past_key_value is not None
            and past_key_value[0].shape[2] == key_value_states.shape[1]
        ):
            # ps: past_key_valueæ˜¯encoder_hidden_statesç»è¿‡k_projå’Œv_projä»¥åŠ_shapeåˆ†å‰²ä¹‹åçš„ç»“æœ(batch_sz,num_heads,encoder_seq_len,head_dim)ï¼Œ key_value_stateså°±æ˜¯encoder_hidden_states,ä¸º(batch_sz,encoder_seq_len,hidden_sz)
            # reuse k,v, cross_attentions
            key_states = past_key_value[0]
            value_states = past_key_value[1]
        elif is_cross_attention:
            # cross_attentions
            # ps: cross attentionéœ€è¦è®¡ç®—keyå’Œvalue
            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)
            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)
        # å‰é¢æ˜¯decoderä¸­çš„cross-attentionéƒ¨åˆ†ï¼Œä¸‹é¢æ˜¯decoderä¸­çš„self-attentionéƒ¨åˆ†
        elif past_key_value is not None:
            # ps: åœ¨self-attentionçš„æƒ…å†µä¸‹ï¼Œå¦‚æœpast_key_valueä¸ä¸ºç©ºï¼Œè¯´æ˜è‡³å°‘æ˜¯ç¬¬äºŒæ¬¡forwardã€‚masked self attentionæ¯ä¸€æ­¥forwardéœ€è¦ä½¿ç”¨çš„å‰é¢çš„keyå’Œvalueï¼Œæ¨å¯¼è§åï¼Œå› æ­¤å¯ä»¥é€šè¿‡ä¿å­˜æ¯æ­¥çš„keyå’ŒvalueèŠ‚çœè®¡ç®—å¼€é”€(kv cache)
            # reuse k, v, self_attention
            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)
            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)
            key_states = torch.cat([past_key_value[0], key_states], dim=2)
            value_states = torch.cat([past_key_value[1], value_states], dim=2)
        else:
            # self_attention
            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)
            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)

        if self.is_decoder:
            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.
            # Further calls to cross_attention layer can then reuse all cross-attention
            # key/value_states (first "if" case)
            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of
            # all previous decoder key/value_states. Further calls to uni-directional self-attention
            # can concat previous decoder key/value_states to current projected key/value_states (third "elif" case)
            # if encoder bi-directional self-attention `past_key_value` is always `None`
            past_key_value = (key_states, value_states)
```
+ åœ¨self-attentionçš„æƒ…å†µä¸‹ï¼Œå¦‚æœpast_key_valueä¸ä¸ºç©ºï¼Œè¯´æ˜è‡³å°‘æ˜¯ç¬¬äºŒæ¬¡forwardã€‚masked self attentionæ¯ä¸€æ­¥forwardéœ€è¦ä½¿ç”¨çš„å‰é¢çš„keyå’Œvalueï¼Œæ¨å¯¼è§åï¼Œå› æ­¤å¯ä»¥é€šè¿‡ä¿å­˜æ¯æ­¥çš„keyå’ŒvalueèŠ‚çœè®¡ç®—å¼€é”€(kv cache) ï¼Œé€‚ç”¨äºdecoderçš„masked self-attention
+ K-V derivation $Q,K,V$:(batch_sz* num_heads,seq_len, head_dim), $K_i,V_i$:(batch_sz* num_heads,1, head_dim)å¯¹åº”ç¬¬iä¸ªtoken(start_tokenä¸ºç¬¬1ä¸ªtoken)çš„embedding(batch_sz,1,hidden_sz)ç»è¿‡å¦‚ä¸Šæ“ä½œåå¾—åˆ°çš„KVï¼Œ ç”Ÿæˆç¬¬i+1ä¸ªtokenæ—¶ï¼Œseq_lenå·²ç»ä¸ºiï¼Œå‰iä¸ªtokençš„embeddingç»„æˆäº†seq embeddingä¸º(batch_sz,i,hidden_sz)ï¼Œç»è¿‡ä¸Šè¿°æ“ä½œåå¾—åˆ°å½“å‰çš„$K,V$Yç”±$K_j,V_j,j=1,...,i$åœ¨seq_lenç»´åº¦ä¸Šæ‹¼æ¥è€Œæˆã€‚

$$
Attn_1(Q,K,V)  = softmaxed(Q_1K_1^T)V_1 
$$
softmaxçš„çŸ©é˜µä¸º(-1,1,1)
$$
\begin{align*}
Attn_2(Q,K,V) & = softmaxed( Q_2(K_1^T, K_2^T)) (V_1,V_2)^T \\
& = softmaxed(Q_2 K_1^T ,Q_2 K_2^T)(V_1,V_2)^T
& = softmaxed(Q_2 K_1^T ,Q_2 K_2^T)
\end{align*}
$$
æ³¨æ„åªæ‹†åˆ†$K,V$ä¸º$k_i,V_i$,$Q$ä¸æ‹†ï¼Œsoftmaxçš„çŸ©é˜µä¸º(-1,2,2),softmaxä½œç”¨äºkeyçš„seq_lenç»´åº¦ï¼Œå³ä¸Šå¼ä¸­çš„è¡Œã€‚(ä¸ºä»€ä¹ˆcat hidden_state?)

```python
        proj_shape = (bsz * self.num_heads, -1, self.head_dim)
        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)
        key_states = key_states.reshape(*proj_shape)
        value_states = value_states.reshape(*proj_shape)
        # ps: (batch_sz, num_heads, seq_len, head_dim) -> (batch_sz*num_heads, seq_len, head_dim) åˆå¹¶å‰ä¸¤ä¸ªç»´åº¦ä¾¿äºåç»­è®¡ç®—ï¼Œå› ä¸ºåˆå¹¶ä¹‹åå‡å°‘ç´¢å¼•æ¬¡æ•°å¸¦æ¥çš„é¢å¤–å¼€é”€ï¼ŒåŒæ—¶åˆå¹¶æˆä¸€ä¸ªå¤§æ‰¹æ¬¡çš„æ•°æ®åï¼Œå†…å­˜è¿ç»­ï¼Œä¹Ÿæ–¹ä¾¿ä¼ è¾“ç»™è®¡ç®—è®¾å¤‡è¿›è¡Œè®¡ç®—

        src_len = key_states.size(1) # ps: seq_len
        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))
        # torch.bmmä¸ºæ‰¹é‡çŸ©é˜µä¹˜æ³• Q:(batch_sz*num_heads,decoder_seq_len,head_dim) * K^T:(batch_sz*num_heads,head_dim,encoder_seq_len) -> attn_weights:(batch_sz*num_heads,seq_len,seq_len)
```
+ (batch_sz, num_heads, seq_len, head_dim) -> (batch_sz*num_heads, seq_len, head_dim) åˆå¹¶å‰ä¸¤ä¸ªç»´åº¦ä¾¿äºåç»­è®¡ç®—ï¼Œå› ä¸ºåˆå¹¶ä¹‹åå‡å°‘ç´¢å¼•æ¬¡æ•°å¸¦æ¥çš„é¢å¤–å¼€é”€ï¼ŒåŒæ—¶åˆå¹¶æˆä¸€ä¸ªå¤§æ‰¹æ¬¡çš„æ•°æ®åï¼Œå†…å­˜è¿ç»­ï¼Œä¹Ÿæ–¹ä¾¿ä¼ è¾“ç»™è®¡ç®—è®¾å¤‡è¿›è¡Œè®¡ç®—
+ torch.bmmä¸ºæ‰¹é‡çŸ©é˜µä¹˜æ³• Q:(batch_sz * num_heads,decoder_seq_len,head_dim) * K^T:(batch_sz* num_heads,head_dim,encoder_seq_len) -> attn_weights:(batch_sz*num_heads,decoder_seq_len,encoder_seq_len)

```python
        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):
            raise ValueError(
                f"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is"
                f" {attn_weights.size()}"
            )
        # æ³¨æ„è¿™é‡Œtgt_lenå³decoder_seq_lenå’Œsrc_lenå³encoder_seq_lenåœ¨ç”Ÿæˆæ—¶å¾€å¾€æ˜¯ä¸åŒçš„ï¼Œtgt_lenæ¯æ¬¡forwardéƒ½ä¼šå¢åŠ ï¼Œsrc_lenä»…ä¸ºinputçš„seq_lenï¼Œæ˜¯ä¿æŒä¸å˜çš„
        if attention_mask is not None:
            if attention_mask.size() != (bsz, 1, tgt_len, src_len):
                raise ValueError(
                    f"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}"
                )
            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask
            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)

        attn_weights = nn.functional.softmax(attn_weights, dim=-1)
```
+ æ³¨æ„è¿™é‡Œtgt_lenå³decoder_seq_lenå’Œsrc_lenå³encoder_seq_lenåœ¨ç”Ÿæˆæ—¶å¾€å¾€æ˜¯ä¸åŒçš„ï¼Œtgt_lenæ¯æ¬¡forwardéƒ½ä¼šå¢åŠ ï¼Œsrc_lenä»…ä¸ºinputçš„seq_lenï¼Œæ˜¯ä¿æŒä¸å˜çš„
+ attention_weightsåŠ ä¸Šattention_maskåç»è¿‡ä¸€å±‚softmaxå†ä¹˜valueï¼Œattention_maskåŒ…æ‹¬padding_maskå’Œcausal_mask,padding maskç”¨äºbatchå†…çš„å¯¹é½ï¼Œcausal_maskåœ¨decoderçš„self-attentionè®­ç»ƒæ—¶ä½¿ç”¨ï¼Œç»´åº¦ä¸º(bsz, 1, tgt_len, tgt_len)ï¼Œä¸ºä¸‹ä¸‰è§’çŸ©é˜µï¼Œcausal[i][j]è¡¨ç¤ºtoken iå¯¹token jçš„æ³¨æ„maskï¼Œå½“i>=jæ—¶ä¸º1ï¼Œè¡¨æ˜token iå¯ä»¥æ³¨æ„åˆ°ä¹‹å‰çš„token jï¼Œå¦åˆ™ä¸º0ã€‚
+ åœ¨å®é™…è®¡ç®—æ—¶ï¼Œattention_maskä¸­çš„1ä½ç½®è¢«æ›¿æ¢æˆ0ï¼Œ0ä½ç½®è¢«æ›¿æ¢æˆ-infï¼Œè¿™æ ·åœ¨softmaxä¹‹åè¯¥ä½ç½®çš„åˆ†æ•°åŸºæœ¬ä¸º0

```python
        if layer_head_mask is not None:
            if layer_head_mask.size() != (self.num_heads,):
                raise ValueError(
                    f"Head mask for a single layer should be of size {(self.num_heads,)}, but is"
                    f" {layer_head_mask.size()}"
                )
            attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(bsz, self.num_heads, tgt_len, src_len)
            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)
        # åˆ©ç”¨broadcastæœºåˆ¶è¿›è¡ŒçŸ©é˜µç‚¹ç§¯ï¼Œlayer_head_maskä¸­å¯èƒ½éœ€è¦maskæ‰æŸäº›æ³¨æ„åŠ›å¤´ï¼ˆå³å¯¹åº”å€¼ä¸º0ï¼‰ï¼Œbroadcaståä¸attn_weightsç›¸ä¹˜å¯ä»¥ç›´æ¥è®©è¯¥headç»´åº¦çš„æ‰€æœ‰å€¼éƒ½ä¸º0
        if output_attentions:
            # this operation is a bit awkward, but it's required to
            # make sure that attn_weights keeps its gradient.
            # In order to do so, attn_weights have to be reshaped
            # twice and have to be reused in the following
            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)
            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)
        else:
            attn_weights_reshaped = None

        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)
        # dropout ä¹‹åä¸valueç›¸ä¹˜

        attn_output = torch.bmm(attn_probs, value_states)
        # (bsz*num_heads,decoder_seq_len,encoder_seq_len) * (bsz*num_heads,encoder_seq_len,head_dim) -> (bsz*num_heads,decoder_seq_len,head_dim)

        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):
            raise ValueError(
                f"`attn_output` should be of size {(bsz * self.num_heads, tgt_len, self.head_dim)}, but is"
                f" {attn_output.size()}"
            )

        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)
        attn_output = attn_output.transpose(1, 2)
        # å¤åŸè‡³(bsz,decoder_seq_len,num_heads,head_dim)

        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be
        # partitioned across GPUs when using tensor-parallelism.
        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)
        # å¤åŸè‡³(bsz,decoder_seq_len, hidden_sz = num_heads * head_dim)

        attn_output = self.out_proj(attn_output)
        # è¾“å‡ºå†projä¸€ä¸‹ï¼Œshapeä¸å˜

        return attn_output, attn_weights_reshaped, past_key_value
```
+ åˆ©ç”¨broadcastæœºåˆ¶è¿›è¡ŒçŸ©é˜µç‚¹ç§¯ï¼Œlayer_head_maskä¸­å¯èƒ½éœ€è¦maskæ‰æŸäº›æ³¨æ„åŠ›å¤´ï¼ˆå³å¯¹åº”å€¼ä¸º0ï¼‰ï¼Œbroadcaståä¸attn_weightsç›¸ä¹˜å¯ä»¥ç›´æ¥è®©è¯¥headç»´åº¦çš„æ‰€æœ‰å€¼éƒ½ä¸º0
+ æœ€åè¾“å‡ºä»ç„¶è¦projä¸€ä¸‹ï¼Œshapeä¸å˜

### Encoder Layeræºç è§£æ
```python
class BartEncoderLayer(nn.Module):
    def __init__(self, config: BartConfig):
        super().__init__()
        self.embed_dim = config.d_model

        self.self_attn = BART_ATTENTION_CLASSES[config._attn_implementation](
            embed_dim=self.embed_dim,
            num_heads=config.encoder_attention_heads,
            dropout=config.attention_dropout,
            config=config,
        )
        self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)
        self.dropout = config.dropout
        self.activation_fn = ACT2FN[config.activation_function] # æ¿€æ´»å‡½æ•°
        self.activation_dropout = config.activation_dropout
        self.fc1 = nn.Linear(self.embed_dim, config.encoder_ffn_dim)
        self.fc2 = nn.Linear(config.encoder_ffn_dim, self.embed_dim)
        self.final_layer_norm = nn.LayerNorm(self.embed_dim)
```
init å‡½æ•°
```python
    def forward(
        self,
        hidden_states: torch.FloatTensor,
        attention_mask: torch.FloatTensor,
        layer_head_mask: torch.FloatTensor,
        output_attentions: Optional[bool] = False,
    ) -> Tuple[torch.FloatTensor, Optional[torch.FloatTensor]]:
        """
        Args:
            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`
            attention_mask (`torch.FloatTensor`): attention mask of size
                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.
            layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size
                `(encoder_attention_heads,)`.
            output_attentions (`bool`, *optional*):
                Whether or not to return the attentions tensors of all attention layers. See `attentions` under
                returned tensors for more detail.
        """
        residual = hidden_states
        hidden_states, attn_weights, _ = self.self_attn(
            hidden_states=hidden_states,
            attention_mask=attention_mask,
            layer_head_mask=layer_head_mask,
            output_attentions=output_attentions,
        )
        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)
        hidden_states = residual + hidden_states
        hidden_states = self.self_attn_layer_norm(hidden_states)

        residual = hidden_states
        hidden_states = self.activation_fn(self.fc1(hidden_states))
        hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)
        hidden_states = self.fc2(hidden_states)
        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)
        hidden_states = residual + hidden_states
        hidden_states = self.final_layer_norm(hidden_states)

        if hidden_states.dtype == torch.float16 and (
            torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any()
        ):
            clamp_value = torch.finfo(hidden_states.dtype).max - 1000
            hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)

        outputs = (hidden_states,)

        if output_attentions:
            outputs += (attn_weights,)

        return outputs
```
### Decoder Layer æºç è§£æ
```python
class BartDecoderLayer(nn.Module):
    def __init__(self, config: BartConfig):
        super().__init__()
        self.embed_dim = config.d_model

        self.self_attn = BART_ATTENTION_CLASSES[config._attn_implementation](
            embed_dim=self.embed_dim,
            num_heads=config.decoder_attention_heads,
            dropout=config.attention_dropout,
            is_decoder=True,
            is_causal=True,
            config=config,
        )
        self.dropout = config.dropout
        self.activation_fn = ACT2FN[config.activation_function]
        self.activation_dropout = config.activation_dropout

        self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)
        self.encoder_attn = BART_ATTENTION_CLASSES[config._attn_implementation](
            self.embed_dim,
            config.decoder_attention_heads,
            dropout=config.attention_dropout,
            is_decoder=True,
            config=config,
        )
        self.encoder_attn_layer_norm = nn.LayerNorm(self.embed_dim)
        self.fc1 = nn.Linear(self.embed_dim, config.decoder_ffn_dim)
        self.fc2 = nn.Linear(config.decoder_ffn_dim, self.embed_dim)
        self.final_layer_norm = nn.LayerNorm(self.embed_dim)
```
init å‡½æ•°
+ æ³¨æ„GPTæ¨¡å‹è™½ç„¶ç§°ä½œdecoder-onlyï¼Œä½†æ˜¯å®é™…ä¸Šåªä¿ç•™äº†multi-head masked self-attentionï¼Œåˆ å»äº†cross-attentionéƒ¨åˆ†

```python
    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        encoder_hidden_states: Optional[torch.Tensor] = None,
        encoder_attention_mask: Optional[torch.Tensor] = None,
        layer_head_mask: Optional[torch.Tensor] = None,
        cross_attn_layer_head_mask: Optional[torch.Tensor] = None,
        past_key_value: Optional[Tuple[torch.Tensor]] = None,
        output_attentions: Optional[bool] = False,
        use_cache: Optional[bool] = True,
    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:
        """
        Args:
            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`
            attention_mask (`torch.FloatTensor`): attention mask of size
                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.
            encoder_hidden_states (`torch.FloatTensor`):
                cross attention input to the layer of shape `(batch, seq_len, embed_dim)`
            encoder_attention_mask (`torch.FloatTensor`): encoder attention mask of size
                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.
            layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size
                `(encoder_attention_heads,)`.
            cross_attn_layer_head_mask (`torch.FloatTensor`): mask for cross-attention heads in a given layer of
                size `(decoder_attention_heads,)`.
            past_key_value (`Tuple(torch.FloatTensor)`): cached past key and value projection states
            output_attentions (`bool`, *optional*):
                Whether or not to return the attentions tensors of all attention layers. See `attentions` under
                returned tensors for more detail.
        """
        residual = hidden_states

        # Self Attention
        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2
        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None
        # add present self-attn cache to positions 1,2 of present_key_value tuple
        hidden_states, self_attn_weights, present_key_value = self.self_attn(
            hidden_states=hidden_states,
            past_key_value=self_attn_past_key_value,
            attention_mask=attention_mask,
            layer_head_mask=layer_head_mask,
            output_attentions=output_attentions,
        )
        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)
        hidden_states = residual + hidden_states
        hidden_states = self.self_attn_layer_norm(hidden_states)
```
self-attention part
```python
        # Cross-Attention Block
        cross_attn_present_key_value = None
        cross_attn_weights = None
        if encoder_hidden_states is not None:
            residual = hidden_states

            # cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple
            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None
            hidden_states, cross_attn_weights, cross_attn_present_key_value = self.encoder_attn(
                hidden_states=hidden_states,
                key_value_states=encoder_hidden_states,
                attention_mask=encoder_attention_mask,
                layer_head_mask=cross_attn_layer_head_mask,
                past_key_value=cross_attn_past_key_value,
                output_attentions=output_attentions,
            )
            hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)
            hidden_states = residual + hidden_states
            hidden_states = self.encoder_attn_layer_norm(hidden_states)

            # add cross-attn to positions 3,4 of present_key_value tuple
            present_key_value = present_key_value + cross_attn_present_key_value

        # Fully Connected
        residual = hidden_states
        hidden_states = self.activation_fn(self.fc1(hidden_states))
        hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)
        hidden_states = self.fc2(hidden_states)
        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)
        hidden_states = residual + hidden_states
        hidden_states = self.final_layer_norm(hidden_states)

        outputs = (hidden_states,)

        if output_attentions:
            outputs += (self_attn_weights, cross_attn_weights)

        if use_cache:
            outputs += (present_key_value,)

        return outputs
```

### GenerationMixin æºç è§£æ
transformersä¸­generationmixinæ˜¯pretrainedmodelçš„çˆ¶ç±»ä¹‹ä¸€ï¼Œæ‰€æœ‰ç”Ÿæˆçš„æ–¹æ³•åœ¨æ­¤å®šä¹‰ã€‚

#### **generateæ–¹æ³•å®šä¹‰**
```python
    @torch.no_grad()
    def generate(
        self,
        inputs: Optional[torch.Tensor] = None,
        generation_config: Optional[GenerationConfig] = None,
        logits_processor: Optional[LogitsProcessorList] = None,
        stopping_criteria: Optional[StoppingCriteriaList] = None,
        prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], List[int]]] = None,
        synced_gpus: Optional[bool] = None,
        assistant_model: Optional["PreTrainedModel"] = None,
        streamer: Optional["BaseStreamer"] = None,
        negative_prompt_ids: Optional[torch.Tensor] = None,
        negative_prompt_attention_mask: Optional[torch.Tensor] = None,
        **kwargs,
    ) -> Union[GenerateOutput, torch.LongTensor]:
```
generateæ–¹æ³•, å‚æ•°è¯¦è§£:
+ inputsä½œä¸ºç”Ÿæˆçš„promptæˆ–encoderçš„è¾“å…¥
+ generation_configç”Ÿæˆçš„å‚æ•°ï¼Œå¦‚do_sample, num_beams, temperature
+ logits_processor è‡ªå®šä¹‰logitså¤„ç†æ¨¡å—
+ stopping_criteria è‡ªå®šä¹‰ç”Ÿæˆåœæ­¢ç­–ç•¥
+ prefix_allowed_tokens_fn é™åˆ¶æ¯æ­¥çš„beam searchä»…å…³æ³¨åœ¨allowed tokenså½“ä¸­ï¼Œè§[Autoregressive EntityRetrieval](https://arxiv.org/abs/2010.00904)
+ synced_gpus æ˜¯å¦ç»§ç»­while loopï¼ŒUnless overridden, this flag will be set to `True` if using `FullyShardedDataParallel` or DeepSpeed ZeRO Stage 3 with multiple GPUs to avoid deadlocking if one GPU finishes generating before other GPUs. Otherwise, defaults to `False`
+ assistant_model ä¸€ä¸ªç”¨äºè¾…åŠ©ç”Ÿæˆçš„å°æ¨¡å‹ï¼Œé€šè¿‡æ›¿ä»£å¤§æ¨¡å‹é¢„æµ‹ä¸‹ä¸€ä¸ªtokenæ¥åŠ é€Ÿæ¨ç†
+ streamer ç”¨äºå¤„ç†è¾“å‡ºsequences

#### **generate-é¢„æ£€æŸ¥**
```python
        # 1. Handle `generation_config` and kwargs that might update it, and validate the `.generate()` call
        self._validate_model_class() # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒgenerate
        tokenizer = kwargs.pop("tokenizer", None)  # Pull this out first, we only use it for stopping criteria
        assistant_tokenizer = kwargs.pop("assistant_tokenizer", None)  # only used for assisted generation
        # ps: kwargs.pop(k,default) -> vï¼Œå¦‚æœæ‰¾åˆ°(k,v)ï¼Œè¿”å›vï¼Œå¦åˆ™è¿”å›default

        generation_config, model_kwargs = self._prepare_generation_config(generation_config, **kwargs)
        self._validate_model_kwargs(model_kwargs.copy())
        self._validate_assistant(assistant_model, tokenizer, assistant_tokenizer)
        # åšä¸€äº›é¢„æ£€æŸ¥

        # 2. Set generation parameters if not already defined
        if synced_gpus is None:
            synced_gpus = (is_deepspeed_zero3_enabled() or is_fsdp_managed_module(self)) and dist.get_world_size() > 1

        logits_processor = logits_processor if logits_processor is not None else LogitsProcessorList()
        stopping_criteria = stopping_criteria if stopping_criteria is not None else StoppingCriteriaList()

        accepts_attention_mask = "attention_mask" in set(inspect.signature(self.forward).parameters.keys()) # é€šè¿‡ç­¾åæ£€æŸ¥forwardæ˜¯å¦æ”¯æŒattention_mask
        requires_attention_mask = "encoder_outputs" not in model_kwargs
        kwargs_has_attention_mask = model_kwargs.get("attention_mask", None) is not None
```
è¿™éƒ¨åˆ†æ˜¯ä¸€äº›é¢„å…ˆå‡†å¤‡å·¥ä½œ
```python
        # 3. Define model inputs
        inputs_tensor, model_input_name, model_kwargs = self._prepare_model_inputs(
            inputs, generation_config.bos_token_id, model_kwargs
        ) # å‡†å¤‡æ¨¡å‹è¾“å…¥
        batch_size = inputs_tensor.shape[0]

        device = inputs_tensor.device
        self._prepare_special_tokens(generation_config, kwargs_has_attention_mask, device=device)

        # decoder-only models must use left-padding for batched generation.
        if not self.config.is_encoder_decoder and not is_torchdynamo_compiling():
            # If `input_ids` was given, check if the last id in any sequence is `pad_token_id`
            # Note: If using, `inputs_embeds` this check does not work, because we want to be more hands-off.
            if (
                generation_config._pad_token_tensor is not None
                and batch_size > 1
                and len(inputs_tensor.shape) == 2
                and torch.sum(inputs_tensor[:, -1] == generation_config._pad_token_tensor) > 0
            ):
                logger.warning(
                    "A decoder-only architecture is being used, but right-padding was detected! For correct "
                    "generation results, please set `padding_side='left'` when initializing the tokenizer."
                )   
```
è¿™éƒ¨åˆ†æ˜¯å¯¹æ¨¡å‹è¾“å…¥çš„å‡†å¤‡ï¼Œè°ƒç”¨äº†_prepare_model_inputsç”¨äºè·å–æ¨¡å‹è¾“å…¥ï¼Œå¦‚æœæŒ‡å®šäº†inputsï¼Œè¿™é‡Œæ²¡æœ‰å˜åŒ–ï¼Œå¦‚æœæ²¡æœ‰æŒ‡å®šinputsï¼Œè¿™é‡Œè¿”å›çš„shapeä¸º(batch_sz,encoder_seq_len/1) åˆ†åˆ«å¯¹åº”encoder-decoderå’Œdecoder-only

#### **_prepare_model_inputsæ–¹æ³•**
```python
    def _prepare_model_inputs(
        self,
        inputs: Optional[torch.Tensor] = None,
        bos_token_id: Optional[torch.Tensor] = None,
        model_kwargs: Optional[Dict[str, torch.Tensor]] = None,
    ) -> Tuple[torch.Tensor, Optional[str], Dict[str, torch.Tensor]]:
        """
        This function extracts the model-specific `inputs` for generation.
        """
        # 1. retrieve all kwargs that are non-None or non-model input related.
        # some encoder-decoder models have different names for model and encoder
        if (
            self.config.is_encoder_decoder
            and hasattr(self, "encoder")
            and self.encoder.main_input_name != self.main_input_name
        ):
            input_name = self.encoder.main_input_name
        else:
            input_name = self.main_input_name

        model_kwargs = {k: v for k, v in model_kwargs.items() if v is not None or k != input_name}
        # æå–æ‰€æœ‰å¯èƒ½å’Œç”Ÿæˆç›¸å…³çš„çš„å‚æ•°

        # 2. check whether model_input_name is passed as kwarg
        # if yes and `inputs` is None use kwarg inputs
        inputs_kwarg = model_kwargs.pop(input_name, None)
        if inputs_kwarg is not None and inputs is not None:
            raise ValueError(
                f"`inputs`: {inputs}` were passed alongside {input_name} which is not allowed. "
                f"Make sure to either pass {inputs} or {input_name}=..."
            )
        elif inputs_kwarg is not None:
            inputs = inputs_kwarg
        # æœ‰å¯èƒ½ä»kwargsä¸­è¾“å…¥äº†

        # 3. In the presence of `inputs_embeds` for text models:
        # - decoder-only models should complain if the user attempts to pass `inputs_embeds`, but the model
        # doesn't have its forwarding implemented. `inputs_embeds` is kept in `model_kwargs` and can coexist with
        # input_ids (`inputs_embeds` will be used in the 1st generation step, as opposed to `input_ids`)
        # - encoder-decoder models should complain if the user attempts to pass `inputs_embeds` and `input_ids`, and
        # pull the former to inputs. It will be used in place of `input_ids` to get the encoder hidden states.
        if input_name == "input_ids" and "inputs_embeds" in model_kwargs:
            if not self.config.is_encoder_decoder:
                has_inputs_embeds_forwarding = "inputs_embeds" in set(
                    inspect.signature(self.prepare_inputs_for_generation).parameters.keys()
                )
                if not has_inputs_embeds_forwarding:
                    raise ValueError(
                        f"You passed `inputs_embeds` to `.generate()`, but the model class {self.__class__.__name__} "
                        "doesn't have its forwarding implemented. See the GPT2 implementation for an example "
                        "(https://github.com/huggingface/transformers/pull/21405), and feel free to open a PR with it!"
                    )
                # In this case, `input_ids` is moved to the `model_kwargs`, so a few automations (like the creation of
                # the attention mask) can rely on the actual model input.
                model_kwargs["input_ids"] = self._maybe_initialize_input_ids_for_generation(
                    inputs, bos_token_id, model_kwargs=model_kwargs
                )
            else:
                if inputs is not None:
                    raise ValueError("You passed `inputs_embeds` and `input_ids` to `.generate()`. Please pick one.")
            inputs, input_name = model_kwargs["inputs_embeds"], "inputs_embeds"

        # 4. if `inputs` is still None, try to create `input_ids` from BOS token
        inputs = self._maybe_initialize_input_ids_for_generation(inputs, bos_token_id, model_kwargs)
        return inputs, input_name, model_kwargs
```
å…¶ä¸­è°ƒç”¨äº†_maybe_initialize_input_ids_for_generationï¼Œå¦‚ä¸‹

#### **_maybe_initialize_input_ids_for_generationæ–¹æ³•**
```python
    def _maybe_initialize_input_ids_for_generation(
        self,
        inputs: Optional[torch.Tensor] = None,
        bos_token_id: Optional[torch.Tensor] = None,
        model_kwargs: Optional[Dict[str, torch.Tensor]] = None,
    ) -> torch.LongTensor:
        """Initializes input ids for generation, if necessary."""
        if inputs is not None:
            return inputs

        encoder_outputs = model_kwargs.get("encoder_outputs") # shapeä¸º(batch_sz,encoder_seq_len,hidden_sz)
        if self.config.is_encoder_decoder and encoder_outputs is not None:
            # make dummy input_ids with value -100, as a sanity check ensuring that they won't be used for encoding
            shape = encoder_outputs.last_hidden_state.size()[:-1] # å³(batch_sz, encoder_seq_len)
            return torch.ones(shape, dtype=torch.long, device=self.device) * -100
        # å¯¹äºencoder-decoderæ¨¡å‹çš„decoderéƒ¨åˆ†ï¼Œç¬¬ä¸€æ¬¡è¾“å…¥forwardæ—¶ç”¨encoder_outputsè¾“å…¥ç»™decoderï¼Œä½†æ˜¯éœ€è¦å°†å®ƒä»¬çš„input_idséƒ½è®¾ä¸ºIGNORE_INDEX=-100

        # If there is some tensor in `model_kwargs`, we can infer the batch size from it. This is helpful with
        # soft-prompting or in multimodal implementations built on top of decoder-only language models.
        batch_size = 1
        for value in model_kwargs.values():
            if isinstance(value, torch.Tensor):
                batch_size = value.shape[0]
                break

        if "inputs_embeds" in model_kwargs:
            return torch.ones((batch_size, 0), dtype=torch.long, device=self.device)

        if bos_token_id is None:
            raise ValueError("`bos_token_id` has to be defined when no `input_ids` are provided.")

        return torch.ones((batch_size, 1), dtype=torch.long, device=self.device) * bos_token_id
```
+ å¦‚æœå·²ç»æœ‰inputsäº†ï¼Œç›´æ¥è¿”å›ï¼Œä¸‹é¢çš„å¤„ç†ä»…é’ˆå¯¹æ²¡æœ‰inputsçš„æƒ…å†µ
+ æ³¨æ„è¿™é‡Œå¯¹äºencoder-decoderæ¨¡å‹çš„decoderéƒ¨åˆ†ï¼Œç¬¬ä¸€æ¬¡è¾“å…¥forwardæ—¶ç”¨encoder_outputsè¾“å…¥ç»™decoderï¼Œä½†æ˜¯éœ€è¦å°†å®ƒä»¬çš„input_idséƒ½è®¾ä¸ºIGNORE_INDEX=-100
+ è€Œå¯¹äºdecoder-onlyæ¨¡å‹,è¿”å›çš„æ˜¯batch_szä¸ªbos_token_idï¼Œä½œä¸ºè¾“å‡ºçš„start tokenï¼Œå¦‚æœå·²ç»æŒ‡å®šäº†inputs_embedsï¼Œåˆ™å•¥éƒ½ä¸è¿”å›

#### **generate-æ¨¡å‹å‚æ•°è¡¥å…¨**
```python
        # 4. Define other model kwargs
        # decoder-only models with inputs_embeds forwarding must use caching (otherwise we can't detect whether we are
        # generating the first new token or not, and we only want to use the embeddings for the first new token)
        if not self.config.is_encoder_decoder and model_input_name == "inputs_embeds":
            generation_config.use_cache = True

        if not kwargs_has_attention_mask and requires_attention_mask and accepts_attention_mask:
            model_kwargs["attention_mask"] = self._prepare_attention_mask_for_generation(
                inputs_tensor, generation_config._pad_token_tensor, generation_config._eos_token_tensor
            )
        elif kwargs_has_attention_mask:
            # TODO (joao): generalize this check with other types of inputs
            if model_input_name == "input_ids" and len(model_kwargs["attention_mask"].shape) > 2:
                raise ValueError("`attention_mask` passed to `generate` must be 2D.")

        if self.config.is_encoder_decoder and "encoder_outputs" not in model_kwargs:
            # if model is encoder decoder encoder_outputs are created and added to `model_kwargs`
            model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
                inputs_tensor, model_kwargs, model_input_name, generation_config
            )
```
+ è¿™é‡Œè°ƒç”¨_prepare_attention_mask_for_generationè·å–attention_maskï¼ˆä»…padä¸º0ï¼Œå…¶ä½™ä¸º1ï¼‰
+ è¿™é‡Œè°ƒç”¨_prepare_encoder_decoder_kwargs_for_generationæ˜¯å¯¹äºencoder-decoderæ¨¡å‹çš„æƒ…å†µï¼Œå¦‚æœæ²¡æœ‰æä¾›encoder_outputsï¼Œè¿™é‡Œè°ƒç”¨encoderçš„forwardä¸€æ¬¡ï¼Œè·å–encoder_outputsï¼Œå­˜å…¥åˆ°model_kwargsä¸­

####  **_prepare_attention_mask_for_generation**
```python
    def _prepare_attention_mask_for_generation(
        self,
        inputs: torch.Tensor,
        pad_token_id: Optional[torch.Tensor],
        eos_token_id: Optional[torch.Tensor],
    ) -> torch.LongTensor:
        # No information for attention mask inference -> return default attention mask
        default_attention_mask = torch.ones(inputs.shape[:2], dtype=torch.long, device=inputs.device)
        if pad_token_id is None:
            return default_attention_mask

        is_input_ids = len(inputs.shape) == 2 and inputs.dtype in [torch.int, torch.long]
        if not is_input_ids:
            return default_attention_mask

        is_pad_token_in_inputs = (pad_token_id is not None) and (
            isin_mps_friendly(elements=inputs, test_elements=pad_token_id).any() 
            # isin_mps_friendly æ˜¯ç”¨æ¥æ£€æŸ¥è¾“å…¥å¼ é‡ä¸­æ˜¯å¦æœ‰pad_token
        )
        is_pad_token_not_equal_to_eos_token_id = (eos_token_id is None) or ~(
            isin_mps_friendly(elements=eos_token_id, test_elements=pad_token_id).any()
        )
        can_infer_attention_mask = is_pad_token_in_inputs * is_pad_token_not_equal_to_eos_token_id
        attention_mask_from_padding = inputs.ne(pad_token_id).long()
        # ï¼šå¦‚æœ inputs ä¸­çš„å…ƒç´ ä¸ pad_token_id ä¸ç›¸ç­‰ï¼Œåˆ™è¯¥ä½ç½®çš„å€¼ä¸º 1ï¼Œè¡¨ç¤ºè¯¥ä½ç½®æœ‰æ•ˆï¼›å¦åˆ™ä¸º 0ï¼Œè¡¨ç¤ºè¯¥ä½ç½®ä¸ºå¡«å……ã€‚inputs.ne(pad_token_id) è¿”å›ä¸€ä¸ªå¸ƒå°”å¼ é‡

        attention_mask = (
            attention_mask_from_padding * can_infer_attention_mask + default_attention_mask * ~can_infer_attention_mask
        )
        return attention_mask
```
+ æœ‰padåˆ™æŠŠpadçš„maskè®¾ä¸º0ï¼Œå…¶ä½™éƒ½ä¸º1

#### **generate-é¢„å¤„ç†è‡ªå›å½’ç”Ÿæˆçš„input_ids**
```python
        # 5. Prepare `input_ids` which will be used for auto-regressive generation
        if self.config.is_encoder_decoder:
            input_ids, model_kwargs = self._prepare_decoder_input_ids_for_generation(
                batch_size=batch_size,
                model_input_name=model_input_name,
                model_kwargs=model_kwargs,
                decoder_start_token_id=generation_config._decoder_start_token_tensor,
                device=inputs_tensor.device,
            )
        else: # decoder-onlyæ¨¡å‹ç›´æ¥ç”¨ä¹‹å‰çš„inputs_tensorï¼Œå³(batch_sz,1)çš„bos_tokenä½œä¸ºinput_ids
            input_ids = inputs_tensor if model_input_name == "input_ids" else model_kwargs.pop("input_ids")

        if generation_config.token_healing:
            input_ids = self.heal_tokens(input_ids, tokenizer)

        if streamer is not None:
            streamer.put(input_ids.cpu())
```
+ _prepare_decoder_input_ids_for_generationåšçš„äº‹æƒ…æ˜¯æŠŠinput_idsæˆ–decoder_input_idsæå–å‡ºæ¥ä½œä¸ºinput_idsï¼Œå¦‚æœæ²¡æœ‰start_tokenï¼Œå†åŠ ä¸Šstart_token


### Trainer æºç è§£æ
transformers version 4.46.1
#### **åˆå§‹åŒ–å‚æ•°**
```python

class Trainer:
    """
    Trainer is a simple but feature-complete training and eval loop for PyTorch, optimized for ğŸ¤— Transformers.

    Args:
        model ([`PreTrainedModel`] or `torch.nn.Module`, *optional*):
            The model to train, evaluate or use for predictions. If not provided, a `model_init` must be passed.

            <Tip>

            [`Trainer`] is optimized to work with the [`PreTrainedModel`] provided by the library. You can still use
            your own models defined as `torch.nn.Module` as long as they work the same way as the ğŸ¤— Transformers
            models.

            </Tip>

        args ([`TrainingArguments`], *optional*):
            The arguments to tweak for training. Will default to a basic instance of [`TrainingArguments`] with the
            `output_dir` set to a directory named *tmp_trainer* in the current directory if not provided.
        data_collator (`DataCollator`, *optional*):
            The function to use to form a batch from a list of elements of `train_dataset` or `eval_dataset`. Will
            default to [`default_data_collator`] if no `processing_class` is provided, an instance of
            [`DataCollatorWithPadding`] otherwise if the processing_class is a feature extractor or tokenizer.
        train_dataset (Union[`torch.utils.data.Dataset`, `torch.utils.data.IterableDataset`, `datasets.Dataset`], *optional*):
            The dataset to use for training. If it is a [`~datasets.Dataset`], columns not accepted by the
            `model.forward()` method are automatically removed.

            Note that if it's a `torch.utils.data.IterableDataset` with some randomization and you are training in a
            distributed fashion, your iterable dataset should either use a internal attribute `generator` that is a
            `torch.Generator` for the randomization that must be identical on all processes (and the Trainer will
            manually set the seed of this `generator` at each epoch) or have a `set_epoch()` method that internally
            sets the seed of the RNGs used.
        eval_dataset (Union[`torch.utils.data.Dataset`, Dict[str, `torch.utils.data.Dataset`, `datasets.Dataset`]), *optional*):
             The dataset to use for evaluation. If it is a [`~datasets.Dataset`], columns not accepted by the
             `model.forward()` method are automatically removed. If it is a dictionary, it will evaluate on each
             dataset prepending the dictionary key to the metric name.
        processing_class (`PreTrainedTokenizerBase` or `BaseImageProcessor` or `FeatureExtractionMixin` or `ProcessorMixin`, *optional*):
            Processing class used to process the data. If provided, will be used to automatically process the inputs
            for the model, and it will be saved along the model to make it easier to rerun an interrupted training or
            reuse the fine-tuned model.
            This supercedes the `tokenizer` argument, which is now deprecated.
        model_init (`Callable[[], PreTrainedModel]`, *optional*):
            A function that instantiates the model to be used. If provided, each call to [`~Trainer.train`] will start
            from a new instance of the model as given by this function.

            The function may have zero argument, or a single one containing the optuna/Ray Tune/SigOpt trial object, to
            be able to choose different architectures according to hyper parameters (such as layer count, sizes of
            inner layers, dropout probabilities etc).
        compute_loss_func (`Callable`, *optional*):
            A function that accepts the raw model outputs, labels, and the number of items in the entire accumulated
            batch (batch_size * gradient_accumulation_steps) and returns the loss. For example, here is one using
            the loss function from `transformers`
        compute_metrics (`Callable[[EvalPrediction], Dict]`, *optional*):
            The function that will be used to compute metrics at evaluation. Must take a [`EvalPrediction`] and return
            a dictionary string to metric values. *Note* When passing TrainingArgs with `batch_eval_metrics` set to
            `True`, your compute_metrics function must take a boolean `compute_result` argument. This will be triggered
            after the last eval batch to signal that the function needs to calculate and return the global summary
            statistics rather than accumulating the batch-level statistics
        callbacks (List of [`TrainerCallback`], *optional*):
            A list of callbacks to customize the training loop. Will add those to the list of default callbacks
            detailed in [here](callback).

            If you want to remove one of the default callbacks used, use the [`Trainer.remove_callback`] method.
        optimizers (`Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]`, *optional*, defaults to `(None, None)`):
            A tuple containing the optimizer and the scheduler to use. Will default to an instance of [`AdamW`] on your
            model and a scheduler given by [`get_linear_schedule_with_warmup`] controlled by `args`.
        preprocess_logits_for_metrics (`Callable[[torch.Tensor, torch.Tensor], torch.Tensor]`, *optional*):
            A function that preprocess the logits right before caching them at each evaluation step. Must take two
            tensors, the logits and the labels, and return the logits once processed as desired. The modifications made
            by this function will be reflected in the predictions received by `compute_metrics`.

            Note that the labels (second parameter) will be `None` if the dataset does not have them.

    Important attributes:

        - **model** -- Always points to the core model. If using a transformers model, it will be a [`PreTrainedModel`]
          subclass.
        - **model_wrapped** -- Always points to the most external model in case one or more other modules wrap the
          original model. This is the model that should be used for the forward pass. For example, under `DeepSpeed`,
          the inner model is wrapped in `DeepSpeed` and then again in `torch.nn.DistributedDataParallel`. If the inner
          model hasn't been wrapped, then `self.model_wrapped` is the same as `self.model`.
        - **is_model_parallel** -- Whether or not a model has been switched to a model parallel mode (different from
          data parallelism, this means some of the model layers are split on different GPUs).
        - **place_model_on_device** -- Whether or not to automatically place the model on the device - it will be set
          to `False` if model parallel or deepspeed is used, or if the default
          `TrainingArguments.place_model_on_device` is overridden to return `False` .
        - **is_in_train** -- Whether or not a model is currently running `train` (e.g. when `evaluate` is called while
          in `train`)

    """
```
å‚æ•°å¦‚ä¸‹ï¼š
- model: PretrainedModel or torch.nn.Moduleï¼Œå¦‚éhuggingface modelï¼Œéœ€è¦æä¾›model_initå‡½æ•°
- args: TrainingArguments
- data_collator: å®šä¹‰å¦‚ä½•å°†å•ä¸ªæ ·æœ¬ç»„è£…æˆä¸€ä¸ªbatchï¼Œå¦‚å¯ä½¿ç”¨DataCollatorWithPaddingæ¥å¤„ç†åŠ¨æ€é•¿åº¦çš„è¾“å…¥
- train_dataset: torch.utils.data.Datasetæˆ–datasets.Dataset
- eval_dataset: å’Œtrain_datasetç±»ä¼¼
- processing_class: é¢„å¤„ç†ç±»ï¼ŒåŒ…æ‹¬`PreTrainedTokenizerBase` or `BaseImageProcessor` or `FeatureExtractionMixin` or `ProcessorMixin`ï¼Œæ€ä¹ˆç”¨å¾…è¿›ä¸€æ­¥äº†è§£
- model_init: Callableï¼Œæ¯æ¬¡`trainer.train()`æ—¶è°ƒç”¨ï¼Œç”¨äºè‡ªå®šä¹‰åˆå§‹åŒ–æ¨¡å‹æ–¹æ³•
- compute_loss_func: Callableï¼Œæ¥æ”¶æ•´ä¸ªaccumulated batchï¼ŒåŒ…æ‹¬æ¨¡å‹åŸå§‹è¾“å‡ºï¼ˆå¦‚logitsï¼‰ã€labelsç­‰ï¼Œè®¡ç®—å¹¶è¿”å›loss(ä¸€ä¸ªå•å…ƒtensorï¼ŒåŒ…å«gradientï¼Œç”¨äºä¹‹åbackward)ï¼Œéœ€è¦è‡ªå®šä¹‰lossæ—¶ä½¿ç”¨
- compute_metrics: Callable, æ¥æ”¶EvalPredictionå‚æ•°ï¼Œé‡Œé¢åŒ…å«å¤„ç†è¿‡åçš„æ¨¡å‹è¾“å‡ºã€labelsç­‰ï¼Œç”¨äºeval metricsçš„è®¡ç®—ã€‚å½“argsä¸­çš„batch_eval_metricsè®¾ä¸ºTrueæ—¶ï¼Œè¿˜é¢å¤–æ¥æ”¶ä¸€ä¸ªcompute_resultå‚æ•°ï¼Œç”¨äºæŒ‡ç¤ºå½“å‰æ˜¯å¦éœ€è¦è®¡ç®—ï¼Œåªæœ‰åœ¨eval batchçš„æœ€åæ‰ä¼šè§¦å‘è®¡ç®—
- callbacks: è‡ªå®šä¹‰callbackï¼Œæ²¡ç”¨è¿‡ï¼Œä¸æ¸…æ¥š
- optimizers: ä¸€ä¸ªtupleï¼ŒåŒ…å«torch.optim.Optimizerå’Œtorch.optim.lr_scheduler.LambdaLRï¼Œé»˜è®¤ä¸ºAdamWå’Œç”¨argsä¸­çš„å‚æ•°è°ƒç”¨get_linear_schedule_with_warmupåˆå§‹åŒ–ä¸€ä¸ªlr_scheduler
- preprocess_logits_for_metrics: Callableï¼Œæ¥æ”¶æ¨¡å‹è¾“å‡ºçš„logitså’Œæ•°æ®çš„labelsï¼Œåœ¨ä¼ ç»™compute_metricsè®¡ç®—å‰è¿›è¡Œé¢„å¤„ç†ï¼Œå…¶è¿”å›å€¼å°†å‡ºç°åœ¨EvalPrediction.predictionsä¸­ã€‚æ­¤å¤–ï¼Œå¯ä»¥è‡ªå®šä¹‰æ­¤å‡½æ•°ç”¨äºè§£å†³evalé˜¶æ®µçˆ†æ˜¾å­˜çš„é—®é¢˜ï¼Œè§[Eval-OOM](/posts/notes/#huggingface-tranier-oom-during-evaluation)

å®˜æ–¹æŒ‡å‡ºçš„ä¸€äº›é‡è¦å±æ€§:
- model: æŒ‡å‘æ ¸å¿ƒæ¨¡å‹ï¼Œå¦‚PretrainedModel
- model_wrapped: å¦‚æœè¢«Deepspeedç­‰åŒ…è£…ï¼Œåˆ™æŒ‡å‘æœ€å¤–å±‚çš„æ¨¡å‹
- is_model_parallel: æ˜¯å¦ä½¿ç”¨æ¨¡å‹å¹¶è¡Œï¼ˆtensor parallelï¼‰
- place_model_on_device: æ§åˆ¶æ¨¡å‹æ˜¯å¦è‡ªåŠ¨æ”¾åˆ°æŒ‡å®šdeviceä¸Šï¼Œå¦‚æœç”¨äº†Deepspeedæˆ–model parallelï¼Œå°†ä¸ºFalse
- is_in_train: æ¨¡å‹æ˜¯å¦åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ˆevaluateä¹Ÿç®—ï¼‰

#### **trainer.train()**
```python
    def train(
        self,
        resume_from_checkpoint: Optional[Union[str, bool]] = None,
        trial: Union["optuna.Trial", Dict[str, Any]] = None,
        ignore_keys_for_eval: Optional[List[str]] = None,
        **kwargs,
    ):
        """
        Main training entry point.

        Args:
            resume_from_checkpoint (`str` or `bool`, *optional*):
                If a `str`, local path to a saved checkpoint as saved by a previous instance of [`Trainer`]. If a
                `bool` and equals `True`, load the last checkpoint in *args.output_dir* as saved by a previous instance
                of [`Trainer`]. If present, training will resume from the model/optimizer/scheduler states loaded here.
            trial (`optuna.Trial` or `Dict[str, Any]`, *optional*):
                The trial run or the hyperparameter dictionary for hyperparameter search.
            ignore_keys_for_eval (`List[str]`, *optional*)
                A list of keys in the output of your model (if it is a dictionary) that should be ignored when
                gathering predictions for evaluation during the training.
            kwargs (`Dict[str, Any]`, *optional*):
                Additional keyword arguments used to hide deprecated arguments
        """
```
åˆå§‹åŒ–å‚æ•°
- resume_from_checkpoint: æŒ‡å®šæ˜¯å¦ä»checkpointå¼€å§‹æ¢å¤è®­ç»ƒ
- trial: è¶…å‚æ•°æœç´¢ç›¸å…³è®¾ç½®ï¼Œæ²¡ç”¨è¿‡ï¼Œä¸äº†è§£
- ignore_keys_for_eval: evalæ—¶å¿½ç•¥æ¨¡å‹è¾“å‡ºçš„æŸäº›keyï¼ˆå¦‚æœè¾“å‡ºå½¢å¼æ˜¯dictç±»å‹çš„è¯ï¼‰

```python
    # from GPT-4o â¤
    if resume_from_checkpoint is False:
        resume_from_checkpoint = None  # å¦‚æœå¸ƒå°”å€¼ä¸ºFalseï¼Œåˆ™ä¸æ¢å¤æ£€æŸ¥ç‚¹

    # åˆå§‹åŒ–å†…å­˜ç›‘æ§
    self._memory_tracker.start()

    args = self.args  # è·å–è®­ç»ƒå‚æ•°é…ç½®

    self.is_in_train = True  # æ ‡è®°ä¸ºè®­ç»ƒçŠ¶æ€

    # å¦‚æœå¯ç”¨äº†NEFTuneå™ªå£°å¢å¼ºï¼Œåˆ™æ¿€æ´»æ¨¡å‹çš„ç›¸å…³é’©å­
    if self.neftune_noise_alpha is not None:
        self.model = self._activate_neftune(self.model)

    # ç¡®ä¿åœ¨æŸäº›æƒ…å†µä¸‹å°†æ¨¡å‹ç§»è‡³è®¾å¤‡ä¸Šï¼ˆå¦‚å¯ç”¨äº†ç‰¹å®šè¯„ä¼°æ¨¡å¼ä½†æœªè®­ç»ƒæ—¶ï¼‰
    if (args.fp16_full_eval or args.bf16_full_eval) and not args.do_train and not self.is_model_parallel:
        self._move_model_to_device(self.model, args.device)

    # å¤„ç†å·²å¼ƒç”¨çš„`model_path`å‚æ•°
    if "model_path" in kwargs:
        resume_from_checkpoint = kwargs.pop("model_path")  # å°†å…¶æ˜ å°„åˆ°resume_from_checkpoint
        warnings.warn(
            "`model_path`å·²å¼ƒç”¨ï¼Œæœªæ¥ç‰ˆæœ¬å°†ç§»é™¤ã€‚è¯·æ”¹ç”¨`resume_from_checkpoint`ã€‚",
            FutureWarning,
        )

    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨æœªé¢„æ–™çš„å…³é”®å­—å‚æ•°ï¼ŒæŠ›å‡ºå¼‚å¸¸
    if len(kwargs) > 0:
        raise TypeError(f"train()æ”¶åˆ°æœªé¢„æœŸçš„å…³é”®å­—å‚æ•°: {', '.join(list(kwargs.keys()))}.")

    # è®¾ç½®è¶…å‚æ•°æœç´¢ç¯å¢ƒï¼ˆåŒ…æ‹¬éšæœºç§å­ç­‰ï¼‰
    self._hp_search_setup(trial)
    self._train_batch_size = self.args.train_batch_size  # åˆå§‹åŒ–è®­ç»ƒæ‰¹é‡å¤§å°

    # æ¨¡å‹é‡æ–°åˆå§‹åŒ–
    model_reloaded = False
    if self.model_init is not None:
        # è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿æ¨¡å‹åˆå§‹åŒ–çš„ç¡®å®šæ€§
        enable_full_determinism(self.args.seed) if self.args.full_determinism else set_seed(self.args.seed)
        self.model = self.call_model_init(trial)  # æ ¹æ®è¶…å‚æ•°é‡æ–°åˆå§‹åŒ–æ¨¡å‹
        model_reloaded = True
        # åŒæ—¶é‡æ–°åˆå§‹åŒ–ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨
        self.optimizer, self.lr_scheduler = None, None

    # å¦‚æœå¯ç”¨äº†æ£€æŸ¥ç‚¹æ¢å¤
    if isinstance(resume_from_checkpoint, bool) and resume_from_checkpoint:
        resume_from_checkpoint = get_last_checkpoint(args.output_dir)  # è·å–æœ€åçš„æ£€æŸ¥ç‚¹
        if resume_from_checkpoint is None:
            raise ValueError(f"è¾“å‡ºç›®å½•({args.output_dir})ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆçš„æ£€æŸ¥ç‚¹")

    if resume_from_checkpoint is not None:
        # åœ¨æ²¡æœ‰ç‰¹å®šåˆ†å¸ƒå¼è®­ç»ƒå¯ç”¨æ—¶ï¼Œç›´æ¥ä»æ£€æŸ¥ç‚¹åŠ è½½æ¨¡å‹çŠ¶æ€
        if not is_sagemaker_mp_enabled() and not self.is_deepspeed_enabled and not self.is_fsdp_enabled:
            self._load_from_checkpoint(resume_from_checkpoint)
        # æ›´æ–°æ‰¹é‡å¤§å°ï¼ˆå¦‚æœé€‚ç”¨ï¼‰
        state = TrainerState.load_from_json(os.path.join(resume_from_checkpoint, TRAINER_STATE_NAME))
        if state.train_batch_size is not None:
            self._train_batch_size = state.train_batch_size

    # å¦‚æœæ¨¡å‹é‡æ–°åˆå§‹åŒ–ï¼Œå°†å…¶ç§»åŠ¨åˆ°ç›®æ ‡è®¾å¤‡å¹¶æ›´æ–°åŒ…è£…æ¨¡å‹
    if model_reloaded:
        if self.place_model_on_device:
            self._move_model_to_device(self.model, args.device)
        self.model_wrapped = self.model  # æ›´æ–°æ¨¡å‹åŒ…è£…å¯¹è±¡

    # æŸ¥æ‰¾é€‚åˆçš„æ‰¹é‡å¤§å°å¹¶è¿è¡Œå†…éƒ¨è®­ç»ƒå¾ªç¯
    inner_training_loop = find_executable_batch_size(
        self._inner_training_loop, self._train_batch_size, args.auto_find_batch_size
    )
    if args.push_to_hub:  # å¦‚æœå¯ç”¨äº†å°†æ¨¡å‹æ¨é€åˆ°Hugging Face Hub
        try:
            hf_hub_utils.disable_progress_bars()  # ä¸´æ—¶ç¦ç”¨è¿›åº¦æ¡
            return inner_training_loop(
                args=args,
                resume_from_checkpoint=resume_from_checkpoint,
                trial=trial,
                ignore_keys_for_eval=ignore_keys_for_eval,
            )
        finally:
            hf_hub_utils.enable_progress_bars()  # ç¡®ä¿è®­ç»ƒå®Œæˆåé‡æ–°å¯ç”¨è¿›åº¦æ¡
    else:
        # æ™®é€šæƒ…å†µä¸‹è¿è¡Œå†…éƒ¨è®­ç»ƒå¾ªç¯
        return inner_training_loop(
            args=args,
            resume_from_checkpoint=resume_from_checkpoint,
            trial=trial,
            ignore_keys_for_eval=ignore_keys_for_eval,
        )

```

#### **trainer._inner_training_loop()**
è®­ç»ƒæµç¨‹ä»£ç 


<!-- ### Q
+ attention_mask æ€ä¹ˆè®¾ç½®ï¼ˆdecoderï¼‰
+ positional embedding
+ k-v cache concat é—®é¢˜
+ ä¸ºä»€ä¹ˆattentionè¦è¿‡ä¸€å±‚softmax
  + éƒ¨åˆ†è§£å†³ï¼šå°†æ ¹æ®QKå¾—åˆ°çš„æ³¨æ„åŠ›åˆ†æ•°è½¬åŒ–ä¸ºæ³¨æ„åŠ›æƒé‡
+ dropoutä½œç”¨  
+ ffnä½œç”¨ï¼Œä¸ºä»€ä¹ˆè¦æ”¾å¤§å†æŠ•å›æ¥
+ word_embedding
+ how to generate  (https://huggingface.co/blog/how-to-generate)
+ generation_config -->

### Positional Embedding
#### **ä½ç½®ç¼–ç **
Transformerä¸­çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶æ— æ³•æ•æ‰ä½ç½®ä¿¡æ¯ï¼Œè¿™æ˜¯å› ä¸ºå…¶è®¡ç®—è¿‡ç¨‹å…·æœ‰ç½®æ¢ä¸å˜æ€§(permutation invariant)ï¼Œå¯¼è‡´æ‰“ä¹±è¾“å…¥åºåˆ—çš„é¡ºåºå¯¹è¾“å‡ºç»“æœä¸ä¼šäº§ç”Ÿä»»ä½•å½±å“ã€‚


[ref](https://0809zheng.github.io/2022/07/01/posencode.html)

### Questions
ref: [æ·±åº¦å­¦ä¹ è‡ªç„¶è¯­è¨€å¤„ç†](https://github.com/DA-southampton/NLP_ability/tree/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86)
#### **ä¸ºä»€ä¹ˆä½¿ç”¨å¤šå¤´æ³¨æ„åŠ› (to do: dim error)**
ä½¿ç”¨å¤šå¤´æ³¨æ„åŠ›å¯ä»¥å­¦ä¹ åˆ°ä¸åŒçš„æ³¨æ„åŠ›æƒé‡ï¼Œå…³æ³¨åˆ°ä¸åŒçš„å­ç©ºé—´ï¼Œå¯ä»¥æ›´å¥½åœ°è·å–è¾“å…¥åºåˆ—ä¸­ä¸åŒä½ç½®çš„å…³ç³»ä¿¡æ¯ã€‚

å…·ä½“æ¥è¯´ï¼Œçœ‹attentionè®¡ç®—å…¬å¼ï¼š

å‡è®¾

$x$: (1,seq_len, hidden_sz), $x_q$: (1,query_len, hidden_sz), n=num_head;

$W_{ki},W_{vi}$: (hidden_sz , head_dim),$ W_{qi}$: (hidden_sz,head_dim); 

$W_k,W_v$: (hidden_sz, n* head_dim=hidden_sz),  $W_q = (W_{q1},W_{q2},..,W{q_n})$: (hidden_sz,hidden_size); 

$$
\begin{align*}
\textbf{each head:} Attn_i &= softmax(\frac{x_q W_{qi} W_{ki}^T x^T}{\sqrt{\text{head_dim}}})W_{vi} x \quad\text{  :(1, query_len, head_dim)} \\
Attn = (Attn_i)_n &= (softmax(\frac{x_q W_{q1} W_{k1}^T x^T}{\sqrt{\text{head_dim}}})W_{v1}x,...,softmax(\frac{x_q W_{qn} W_{kn}^T x^T}{\sqrt{\text{head_dim}}})W_{vn}x) \quad\text{  :(1, query_len, n* head_dim=hidden_sz)}
\end{align*}
$$
$$
\begin{align*}
\textbf{no head}: Attn &= softmax(\frac{x_q W_q W_k^T x^T}{\sqrt{\text{hidden_sz}}}) W_v x \\
& = softmax(\frac{\sum_{i=1}^n x_q W_{qi} W_{ki}^T x^T}{\sqrt{\text{hidden_sz}}}) W_v x\\
& = (softmax(\frac{\sum_{i=1}^n x_q W_{qi} W_{ki}^T x^T}{\sqrt{\text{hidden_sz}}}) W_{v1} x, ..., softmax(\frac{\sum_{i=1}^n x_q W_{qi} W_{ki}^T x^T}{\sqrt{\text{hidden_sz}}}) W_{vn} x) \quad\text{  :(1, query_len, hidden_sz)}
\end{align*}
$$
å¤šå¤´æ³¨æ„åŠ›çš„æƒ…å†µä¸‹ï¼Œæ¯ä¸ªå¤´çš„æ³¨æ„åŠ›æƒé‡æ˜¯ä¸åŒçš„ï¼Œå› æ­¤å¯ä»¥å…³æ³¨åˆ°ä¸åŒå­ç©ºé—´çš„ä¿¡æ¯ï¼›å•å¤´æ³¨æ„åŠ›æƒ…å†µä¸‹è™½ç„¶æœ€ç»ˆè¾“å‡ºç»´åº¦ç›¸åŒï¼Œä½†æŠŠhidden_szç»´åº¦æŒ‰ç…§num_head*head_dimæ–¹å¼åˆ‡åˆ†åå¯ä»¥å‘ç°åŒä¸€ä¸ªæ³¨æ„åŠ›æƒé‡é‡å¤äº†num_headæ¬¡

#### **self-attention ä¸ºä»€ä¹ˆQ,K,Vä½¿ç”¨ä¸åŒçš„æƒé‡çŸ©é˜µç”Ÿæˆï¼Œä¸ºä½•ä¸èƒ½ä½¿ç”¨åŒä¸€ä¸ªå€¼è¿›è¡Œè‡ªèº«çš„ç‚¹ä¹˜ï¼Ÿ**
å¯ä»¥åœ¨ä¸åŒç©ºé—´è¿›è¡ŒæŠ•å½±ï¼Œæå–åˆ°æ›´å¤šä¿¡æ¯ã€‚ç›¸åŒæƒé‡æ¨¡å‹å¯èƒ½æ— æ³•å¾ˆå¥½åŒºåˆ†Q,K,V

#### **è®¡ç®—attentionæ—¶ä¸ºä½•é€‰æ‹©ç‚¹ä¹˜è€Œä¸æ˜¯åŠ æ³•ï¼Ÿä¸¤è€…åœ¨è®¡ç®—å¤æ‚åº¦å’Œæ•ˆæœä¸Šæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ**
ç‚¹ä¹˜å¯ä»¥é€šè¿‡çŸ©é˜µä¹˜æ³•çš„æ–¹å¼è¿›è¡Œå¹¶è¡Œè®¡ç®—ä¼˜åŒ–ï¼Œæ¯”çŸ©é˜µåŠ æ³•çš„å¹¶è¡ŒåŒ–å®ç°æ›´å®¹æ˜“æ›´é«˜æ•ˆã€‚ç†è®ºå¤æ‚åº¦ä¸€æ ·ï¼Œä½†å®é™…ä¸ŠåŠ æ³•ä¹‹åçš„éçº¿æ€§æ¿€æ´»å‡½æ•°è¾ƒéš¾å¹¶è¡Œï¼Œå› æ­¤æ•ˆæœæ›´å·®ã€‚

ä»æ•°å­¦è§’åº¦çœ‹ï¼Œç‚¹ä¹˜æ˜¯ä¸€ç§è¡¡é‡ä¸¤ä¸ªå‘é‡ç›¸ä¼¼åº¦çš„è‡ªç„¶æ–¹å¼ã€‚å½“æŸ¥è¯¢ Q å’Œé”® K çš„æ–¹å‘ç›¸ä¼¼æ—¶ï¼Œç‚¹ç§¯å€¼ä¼šè¾ƒå¤§ï¼Œsoftmaxåçš„æƒé‡ä¹Ÿä¼šè¾ƒå¤§ï¼Œæ„å‘³ç€è¿™ç§ç›¸ä¼¼æ€§ç›´æ¥å½±å“äº†æ³¨æ„åŠ›æƒé‡çš„å¤§å°ã€‚è¿™ç§ç›´æ¥ä½¿ç”¨ç›¸ä¼¼æ€§è¿›è¡Œæƒé‡åˆ†é…çš„æ–¹å¼éå¸¸ç›´è§‚ä¸”é«˜æ•ˆã€‚

#### **ä¸ºä»€ä¹ˆåœ¨è¿›è¡Œsoftmaxä¹‹å‰éœ€è¦å¯¹attentionè¿›è¡Œscaledï¼ˆä¸ºä»€ä¹ˆé™¤ä»¥dkçš„å¹³æ–¹æ ¹ï¼‰ï¼Œå¹¶ä½¿ç”¨å…¬å¼æ¨å¯¼è¿›è¡Œè®²è§£**
[ref](https://blog.csdn.net/ytusdc/article/details/121622205)

#### **åœ¨è®¡ç®—attention scoreçš„æ—¶å€™å¦‚ä½•å¯¹paddingåšmaskæ“ä½œï¼Ÿ**
æ ¹æ®attetnion maskçš„æ ‡è®°ï¼Œå°†ä¸è¢«æ³¨æ„çš„ä½ç½®ï¼ˆmaskä¸º0ï¼‰çš„å€¼éƒ½è®¾ä¸ºè¾ƒå¤§çš„è´Ÿå€¼(å¦‚-100)ï¼Œè¿™æ ·ç»è¿‡softmaxä¹‹åå‡ ä¹å°±åŸºæœ¬ç­‰äº0ï¼Œä¹Ÿå°±ä¸ä¼šè®¡ç®—è¯¥ä½ç½®çš„attention score
(to do: code analysis)

#### **ç®€å•è®²ä¸€ä¸‹Transformerä¸­çš„æ®‹å·®ç»“æ„ä»¥åŠæ„ä¹‰.**
æ®‹å·®ç»“æ„å¹¿æ³›è®¤ä¸ºç”±ResNetå¼•å…¥ï¼Œä¸»è¦ä½œç”¨ä¸ºè§£å†³æ¢¯åº¦æ¶ˆå¤±å’Œæƒé‡çŸ©é˜µé€€åŒ–çš„é—®é¢˜ã€‚

æ¢¯åº¦æ¶ˆå¤±æ˜¯å› ä¸ºæ ¹æ®é“¾å¼æ³•åˆ™ï¼Œæ¢¯åº¦æ˜¯ç›¸ä¹˜çš„ï¼Œä¸€æ—¦æŸäº›é¡¹æ¢¯åº¦å¾ˆå°ï¼Œæ·±åº¦ç½‘ç»œè¿ä¹˜ä¹‹åæ•´ä¸ªæ¢¯åº¦ä¼šå˜å¾—éå¸¸å°ã€‚åŠ ä¸Šæ®‹å·®ç»“æ„ä½¿å¾—æ¯é¡¹æ¢¯åº¦å˜ä¸º(1+grad)ï¼Œé¿å…äº†æ¢¯åº¦æ¶ˆå¤±ã€‚

æƒé‡çŸ©é˜µé€€åŒ–æ˜¯å› ä¸ºè™½ç„¶æ¢¯åº¦èŒƒæ•°å¤§ï¼Œä½†æ˜¯å¦‚æœç½‘ç»œçš„å¯ç”¨è‡ªç”±åº¦å¯¹è¿™äº›èŒƒæ•°çš„è´¡çŒ®éå¸¸ä¸å‡è¡¡ï¼Œä¹Ÿå°±æ˜¯æ¯ä¸ªå±‚ä¸­åªæœ‰å°‘é‡çš„éšè—å•å…ƒå¯¹ä¸åŒçš„è¾“å…¥æ”¹å˜å®ƒä»¬çš„æ¿€æ´»å€¼ï¼Œè€Œå¤§éƒ¨åˆ†éšè—å•å…ƒå¯¹ä¸åŒçš„è¾“å…¥éƒ½æ˜¯ç›¸åŒçš„ååº”ï¼Œæ­¤æ—¶æ•´ä¸ªæƒé‡çŸ©é˜µçš„ç§©ä¸é«˜ã€‚å¹¶ä¸”éšç€ç½‘ç»œå±‚æ•°çš„å¢åŠ ï¼Œè¿ä¹˜åä½¿å¾—æ•´ä¸ªç§©å˜çš„æ›´ä½ã€‚è™½ç„¶æ˜¯ä¸€ä¸ªå¾ˆé«˜ç»´çš„çŸ©é˜µï¼Œä½†æ˜¯å¤§éƒ¨åˆ†ç»´åº¦å´æ²¡æœ‰ä¿¡æ¯ï¼Œè¡¨è¾¾èƒ½åŠ›æ²¡æœ‰çœ‹èµ·æ¥é‚£ä¹ˆå¼ºå¤§ã€‚æ®‹å·®è¿æ¥æ­£æ˜¯å¼ºåˆ¶æ‰“ç ´äº†ç½‘ç»œçš„å¯¹ç§°æ€§ï¼Œä¸€å®šç¨‹åº¦ä¸Šç¼“è§£äº†çŸ©é˜µä½ç§©çš„é—®é¢˜ï¼Œæå‡äº†ç½‘ç»œçš„è¡¨å¾èƒ½åŠ›ã€‚

[ref](https://zhuanlan.zhihu.com/p/42833949)

#### **ä¸ºä»€ä¹ˆtransformerå—ä½¿ç”¨LayerNormè€Œä¸æ˜¯BatchNormï¼ŸLayerNorm åœ¨Transformerçš„ä½ç½®æ˜¯å“ªé‡Œï¼Ÿ**
Transformerä½¿ç”¨LayerNormè€ŒéBatchNormæ˜¯å› ä¸ºLayerNormå¯¹æ¯ä¸ªæ ·æœ¬ç‹¬ç«‹è¿›è¡Œå½’ä¸€åŒ–ï¼Œé€‚åˆå˜é•¿è¾“å…¥åºåˆ—çš„å¤„ç†ã€‚è€ŒBatchNormåœ¨åºåˆ—å»ºæ¨¡ä¸­ä¼šå—åˆ°æ‰¹æ¬¡å¤§å°å’Œåºåˆ—é•¿åº¦çš„å½±å“ï¼Œå¯¼è‡´ä¸ç¨³å®šã€‚

LayerNormé€šå¸¸æ”¾ç½®åœ¨æ¯ä¸ªå­å±‚çš„è¾“å‡ºä¹‹åã€‚å³attentionå’Œfeed forwardä¹‹å

#### **Encoderç«¯å’ŒDecoderç«¯æ˜¯å¦‚ä½•è¿›è¡Œäº¤äº’çš„ï¼Ÿ**
åœ¨decoderçš„cross-attentionæ¨¡å—è¿›è¡Œäº¤äº’ï¼Œencoderæœ€åè¾“å‡ºçš„hidden_statesä½œä¸ºcross-attentionçš„keyå’Œvalueï¼Œdecoderçš„self-attentionæ¨¡å—è¾“å‡ºçš„decoder_hidden_statesä½œä¸ºqueryï¼Œè¿›è¡Œcross-attentionå®ç°äº¤äº’

#### **Decoderé˜¶æ®µçš„å¤šå¤´è‡ªæ³¨æ„åŠ›å’Œencoderçš„å¤šå¤´è‡ªæ³¨æ„åŠ›æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ**
Decoderé˜¶æ®µçš„å¤šå¤´è‡ªæ³¨æ„åŠ›éœ€è¦è¿›è¡Œåºåˆ—maskæ“ä½œï¼Œä»¥é˜²æ­¢æ¨¡å‹åœ¨ç”Ÿæˆå½“å‰è¯æ—¶æŸ¥çœ‹æœªæ¥çš„è¯ã€‚è€ŒEncoderçš„å¤šå¤´è‡ªæ³¨æ„åŠ›åˆ™ä¸éœ€è¦maskï¼Œå› ä¸ºå®ƒå¯ä»¥åŒæ—¶çœ‹åˆ°è¾“å…¥åºåˆ—çš„æ‰€æœ‰ä¿¡æ¯ã€‚

#### **Transformerçš„å¹¶è¡ŒåŒ–æç°åœ¨å“ªä¸ªåœ°æ–¹ï¼ŸDecoderç«¯å¯ä»¥åšå¹¶è¡ŒåŒ–å—ï¼Ÿ**
Transformerçš„å¹¶è¡ŒåŒ–ä¸»è¦ä½“ç°åœ¨Encoderçš„å¤šä¸ªå±‚å’Œå¤šå¤´æ³¨æ„åŠ›æœºåˆ¶çš„å¹¶è¡Œè®¡ç®—ä¸Šã€‚Decoderç«¯åœ¨ç”Ÿæˆåºåˆ—æ—¶ï¼Œç”±äºéœ€è¦ä¾èµ–å‰ä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡ºï¼Œé€šå¸¸éš¾ä»¥å®Œå…¨å¹¶è¡ŒåŒ–ï¼Œä½†åœ¨Decoderçš„æ¯å±‚å†…éƒ¨ä»å¯ä»¥è¿›è¡Œå¹¶è¡Œå¤„ç†ã€‚



## Megatron
### æ•°æ®å¹¶è¡Œ
åœ¨æ¯ä¸ªworkerä¹‹ä¸Šå¤åˆ¶ä¸€ä»½æ¨¡å‹ï¼Œè¿™æ ·æ¯ä¸ªworkeréƒ½æœ‰ä¸€ä¸ªå®Œæ•´æ¨¡å‹çš„å‰¯æœ¬ã€‚è¾“å…¥æ•°æ®é›†æ˜¯åˆ†ç‰‡çš„ï¼Œä¸€ä¸ªè®­ç»ƒçš„å°æ‰¹é‡æ•°æ®å°†åœ¨å¤šä¸ªworkerä¹‹é—´åˆ†å‰²ï¼›workerå®šæœŸæ±‡æ€»å®ƒä»¬çš„æ¢¯åº¦ï¼Œä»¥ç¡®ä¿æ‰€æœ‰workerçœ‹åˆ°ä¸€ä¸ªä¸€è‡´çš„æƒé‡ç‰ˆæœ¬ã€‚å¯¹äºæ— æ³•æ”¾è¿›å•ä¸ªworkerçš„å¤§å‹æ¨¡å‹ï¼Œäººä»¬å¯ä»¥åœ¨æ¨¡å‹ä¹‹ä¸­è¾ƒå°çš„åˆ†ç‰‡ä¸Šä½¿ç”¨æ•°æ®å¹¶è¡Œã€‚

æ•°æ®å¹¶è¡Œæ‰©å±•é€šå¸¸æ•ˆæœå¾ˆå¥½ï¼Œä½†æœ‰ä¸¤ä¸ªé™åˆ¶ï¼š

aï¼‰è¶…è¿‡æŸä¸€ä¸ªç‚¹ä¹‹åï¼Œæ¯ä¸ªGPUçš„batch sizeå˜å¾—å¤ªå°ï¼Œè¿™é™ä½äº†GPUçš„åˆ©ç”¨ç‡ï¼Œå¢åŠ äº†é€šä¿¡æˆæœ¬ï¼›

bï¼‰å¯ä½¿ç”¨çš„æœ€å¤§è®¾å¤‡æ•°å°±æ˜¯batch sizeï¼Œç€é™åˆ¶äº†å¯ç”¨äºè®­ç»ƒçš„åŠ é€Ÿå™¨æ•°é‡ã€‚

åŒä¸€ä¸ªData Parallel Groupå†…çš„æ•°æ®æ˜¯ä¸åŒçš„ï¼Œç›¸å½“äºæŠŠæ•´ä¸ªè¾“å…¥æ•°æ®åˆ‡åˆ†æˆdp sizeä¸ªDP group

### æ¨¡å‹å¹¶è¡Œ
æ¨¡å‹å¹¶è¡Œæ¨¡å¼ä¼šè®©ä¸€ä¸ªæ¨¡å‹çš„å†…å­˜å’Œè®¡ç®—åˆ†å¸ƒåœ¨å¤šä¸ªworkerä¹‹é—´ï¼Œä»¥æ­¤æ¥è§£å†³ä¸€ä¸ªæ¨¡å‹åœ¨ä¸€å¼ å¡ä¸Šæ— æ³•å®¹çº³çš„é—®é¢˜ï¼Œå…¶è§£å†³æ–¹æ³•æ˜¯æŠŠæ¨¡å‹æ”¾åˆ°å¤šä¸ªè®¾å¤‡ä¹‹ä¸Šã€‚

æ¨¡å‹å¹¶è¡Œåˆ†ä¸ºä¸¤ç§ï¼šæµæ°´çº¿å¹¶è¡Œå’Œå¼ é‡å¹¶è¡Œï¼Œå°±æ˜¯æŠŠæ¨¡å‹åˆ‡åˆ†çš„æ–¹å¼ã€‚

æµæ°´çº¿å¹¶è¡Œï¼ˆpipeline model parallelï¼‰æ˜¯æŠŠæ¨¡å‹ä¸åŒçš„å±‚æ”¾åˆ°ä¸åŒè®¾å¤‡ä¹‹ä¸Šï¼Œæ¯”å¦‚å‰é¢å‡ å±‚æ”¾åˆ°ä¸€ä¸ªè®¾å¤‡ä¹‹ä¸Šï¼Œä¸­é—´å‡ å±‚æ”¾åˆ°å¦å¤–ä¸€ä¸ªè®¾å¤‡ä¸Šï¼Œæœ€åå‡ å±‚æ”¾åˆ°ç¬¬ä¸‰ä¸ªè®¾å¤‡ä¹‹ä¸Šã€‚

å¼ é‡å¹¶è¡Œåˆ™æ˜¯å±‚å†…åˆ†å‰²ï¼ŒæŠŠæŸä¸€ä¸ªå±‚åšåˆ‡åˆ†ï¼Œæ”¾ç½®åˆ°ä¸åŒè®¾å¤‡ä¹‹ä¸Šï¼Œä¹Ÿå¯ä»¥ç†è§£ä¸ºæŠŠçŸ©é˜µè¿ç®—åˆ†é…åˆ°ä¸åŒçš„è®¾å¤‡ä¹‹ä¸Šï¼Œæ¯”å¦‚æŠŠæŸä¸ªçŸ©é˜µä¹˜æ³•åˆ‡åˆ†æˆä¸ºå¤šä¸ªçŸ©é˜µä¹˜æ³•æ”¾åˆ°ä¸åŒè®¾å¤‡ä¹‹ä¸Šã€‚

#### **é€šä¿¡**
æˆ‘ä»¬æ¥ä¸‹æ¥çœ‹çœ‹æ¨¡å‹å¹¶è¡Œçš„é€šä¿¡çŠ¶å†µã€‚

å¼ é‡å¹¶è¡Œï¼šé€šä¿¡å‘ç”Ÿåœ¨æ¯å±‚çš„å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­è¿‡ç¨‹ä¹‹ä¸­ï¼Œé€šä¿¡ç±»å‹æ˜¯all-reduceï¼Œä¸ä½†å•æ¬¡é€šä¿¡æ•°æ®é‡å¤§ï¼Œå¹¶ä¸”é€šä¿¡é¢‘ç¹(ä¸€æ¬¡forward+backwardéœ€è¦4æ¬¡all-reduce)ã€‚

æµæ°´çº¿å¹¶è¡Œï¼šé€šä¿¡åœ¨æµæ°´çº¿é˜¶æ®µç›¸é‚»çš„åˆ‡åˆ†ç‚¹ä¹‹ä¸Šï¼Œé€šä¿¡ç±»å‹æ˜¯P2Pé€šä¿¡ï¼Œå•æ¬¡é€šä¿¡æ•°æ®é‡è¾ƒå°‘ä½†æ˜¯æ¯”è¾ƒé¢‘ç¹ï¼Œè€Œä¸”å› ä¸ºæµæ°´çº¿çš„ç‰¹ç‚¹ï¼Œä¼šäº§ç”ŸGPUç©ºé—²æ—¶é—´ï¼Œè¿™é‡Œç§°ä¸ºæµæ°´çº¿æ°”æ³¡ï¼ˆBubbleï¼‰ã€‚

å› ä¸ºå¼ é‡å¹¶è¡Œä¸€èˆ¬éƒ½åœ¨åŒä¸€ä¸ªæœºå™¨ä¹‹ä¸Šï¼Œæ‰€ä»¥é€šè¿‡ NVLink æ¥è¿›è¡ŒåŠ é€Ÿï¼Œå¯¹äºæµæ°´çº¿å¹¶è¡Œï¼Œä¸€èˆ¬é€šè¿‡ Infiniband äº¤æ¢æœºè¿›è¡Œè¿æ¥ã€‚

#### **MLPï¼ˆfeedforwardï¼‰éƒ¨åˆ†åˆ‡åˆ†æ–¹æ³•**
åˆ‡åˆ†æ–¹æ³•å¦‚å›¾æ‰€ç¤º
![megatron-mlp-parallel](images/notes/megatron-mlp-parallel.png)
å‡è®¾Y=ACT(XA)ï¼Œå¦‚æœAæ²¿è¡Œåˆ‡ï¼Œé‚£ä¹ˆéœ€è¦Xæ²¿åˆ—åˆ‡ï¼Œæœ€ç»ˆå¾—åˆ°Y=ACT(X1A1+X2A2)ï¼Œç”±äºACTçš„éçº¿æ€§ï¼Œè¿™é‡ŒYä¸ç­‰äºACT(X1A1)+ACT(X2A2)ï¼Œå› æ­¤éœ€è¦reduceä¸€æ¬¡æ‰èƒ½è®¡ç®—Yï¼Œæ²¡æ³•å¹¶è¡Œ

ä½†å¦‚æœAæ²¿åˆ—åˆ‡ï¼Œåˆ™Y=ACT(XA1,XA2)ï¼ŒACTä½œç”¨äºæœ€åä¸€ç»´hidden_szçš„æ¯ä¸ªå…ƒç´ ä¸Šï¼Œè¿™æ ·é€šè¿‡å¹¶è¡Œåæ‹¼æ¥å¯ä»¥å®ç°æ¿€æ´»å‡½æ•°çš„å¹¶è¡Œï¼Œå› æ­¤éœ€è¦å°†æƒé‡å‡½æ•°æ²¿åˆ—åˆ‡ï¼ˆå³æ²¿æœ€åä¸€ç»´åˆ‡ï¼‰

X:(bz,seq_len,hidden_sz), A:(hidden_sz,ffn_hidden_sz), Ai:(hidden_sz,ffn_hidden_sz_i)

è¿™æ˜¯ç¬¬ä¸€ä¸ªLinear+æ¿€æ´»å‡½æ•°çš„å¹¶è¡Œæ–¹æ³•ï¼Œä¸Šä¸€æ­¥å¹¶è¡Œåˆ†åˆ«åœ¨ä¸¤ä¸ªGPUä¸Šå¾—åˆ°Y=(Y1,Y2)ï¼Œä¸‹ä¸€æ­¥éœ€è¦ç»è¿‡å¦ä¸€ä¸ªçº¿æ€§å±‚ï¼ŒZ=DROPOUT(YB),åˆšå¥½Yæ˜¯åˆ—åˆ‡ï¼Œé‚£ä¹ˆå°†Bè¡Œåˆ‡æˆB1å’ŒB2å³å¯,
Z=DROPOUT(Y1B1+Y2B2),åœ¨è¿™é‡Œåšreduceå¾—åˆ°è¾“å‡ºZ

#### **self-attentionéƒ¨åˆ†åˆ‡åˆ†æ–¹æ³•**
ç›´æ¥æŒ‰æ³¨æ„åŠ›å¤´åˆ‡å³å¯

#### **æ¢¯åº¦ä¼ å¯¼**
(to do: more)
çŸ©é˜µæ±‚å¯¼åˆ†å‰²è½¬åŒ–

### å¹¶è¡Œé…ç½®
#### **å‚æ•°è§£é‡Š**
+ p: pplp size
+ t: tp size
+ d: dp size
+ n: num of gpus = p * t * d
+ B: global batch size
+ b: micro batch size
+ m $=\frac{B}{b *  d}$ num of microbatches per pplï¼Œå½“mä¸º1æ—¶ï¼Œç›¸å½“äºB=b*dï¼Œå³å¯¹global batchæ•°æ®æŒ‰dè¿›è¡Œåˆ‡åˆ†ï¼Œæ¯ä¸ªdpç»„å†…çš„micro batch sizeä¸ºB/d



#### **Example**
+ n = 16 = {node1:0-7,node2:8-15}
+ tp = 2
+ pp = 4

åˆ†ç»„ä¸º
TP: group sizeä¸º2ï¼Œå…±8ä¸ªç»„: [0,1],[2,3],[4,5],...,[14,15]ï¼Œæ¯ä¸ªgroupè¡¨ç¤ºä¸€ç»„å¼ é‡å¹¶è¡Œï¼Œtpçš„é€šä¿¡ä»…åœ¨ç»„å†…è¿›è¡Œ
PP: group sizeä¸º4ï¼Œå…±4ä¸ªç»„: [0,4,8,12],[1,5,9,13],[2,6,10,14],[3,7,11,15], æ¯ä¸ªgroupè¡¨ç¤ºä¸€ç»„æµæ°´çº¿å¹¶è¡Œï¼Œppä¼˜å…ˆæœºé—´è¿›è¡Œï¼Œppçš„p2pé€šä¿¡ä»…åœ¨ç»„å†…å®Œæˆ
DP: n/(tp*pp) = 2ï¼Œè¯´æ˜å¤åˆ¶äº†ä¸¤ä¸ªæ¨¡å‹ï¼Œdpä¸º2ï¼Œgroup sizeä¸º2ï¼Œå…±8ä¸ªç»„: [0,2],[1,3],[4,6],[5,7],...,[13,15]ï¼Œæ¯ä¸ªgroupè¡¨ç¤ºä¸€ç»„æ•°æ®å¹¶è¡Œï¼Œç»„å†…å„GPUçš„æ•°æ®ä¸åŒï¼Œä¸€ä¸ªç»„çš„æ•°æ®åˆå¹¶ä¹‹åä¸ºglobal data

## Deepspeed
### æ˜¾å­˜å ç”¨
+ å‡è®¾æ¨¡å‹å‚æ•°é‡ä¸ºMï¼Œæ•°æ®æ ¼å¼ä¸ºfp16ï¼Œåˆ™ç°å­˜ä¸º2M Bytesã€‚ 
+ æ¯ä¸ªå‚æ•°å¯¹åº”ä¸€ä¸ªæ¢¯åº¦ï¼Œæ‰€ä»¥æ¢¯åº¦ä¸º2M
+ Adamä¼˜åŒ–å™¨
  + åŒ…å«fp32çš„å‚æ•°å¤‡ä»½ï¼Œå¤§å°ä¸º4M
  + fp32çš„ä¸€é˜¶momentumï¼Œå¤§å°ä¸º4M
  + fp32çš„äºŒé˜¶varianceï¼Œå¤§å°ä¸º4M
+ æ€»è®¡16M Bytes
+ æ­¤å³æ‰€è°“æ··åˆç²¾åº¦è®­ç»ƒ

### Zero Stages
+ Baseline
    + æ¯ä¸ªGPUä¸Šå­˜å‚æ•°+æ¢¯åº¦+ä¼˜åŒ–å™¨å‚æ•°
    + æ¯å¡16M
    + æ¯ä¸ªstepåè¿›è¡Œä¸€æ¬¡all-reduceè®¡ç®—æ¢¯åº¦å‡å€¼ï¼Œæ ¹æ®[ç¯çŠ¶é€šä¿¡](https://zhuanlan.zhihu.com/p/504957661)ï¼Œå¯¹æ¯å¼ å¡è€Œè¨€ï¼Œå‘é€åŠ æ¥æ”¶çš„æ€»é€šä¿¡æ•°æ®é‡è¿‘ä¼¼ä¸º2M
+ Stage 1
    + å¯¹ä¼˜åŒ–å™¨å‚æ•°è¿›è¡Œåˆ‡åˆ†ï¼ŒNä¸ªGPUæ¯ä¸ªGPUä¿å­˜1/Nçš„ä¼˜åŒ–å™¨çŠ¶æ€é‡ï¼Œåˆå¹¶ä¸€èµ·æˆä¸ºä¸€ä¸ªæ€»çš„ä¼˜åŒ–å™¨çŠ¶æ€é‡
    + æ¯å¡4M+12M/N
    + é€šä¿¡é‡åŒstage2
+ Stage 2
    + å¯¹æ¨¡å‹æ¢¯åº¦è¿›è¡Œåˆ‡åˆ†ï¼Œæ¯ä¸ªGPUä¿å­˜1/Næ¢¯åº¦
    + æ¯å¡2M + 14M/n
    + æ¯å¡è®¡ç®—1/Næ¢¯åº¦å‡å€¼ï¼Œéœ€è¦ä¸€æ¬¡reduceï¼Œé€šä¿¡é‡M; ç®—å®Œæ¢¯åº¦æ›´æ–°ä¼˜åŒ–å™¨çŠ¶æ€ï¼Œéœ€è¦ä¸€æ¬¡gather,é€šä¿¡å½•M
+ Stage 3
    + å¯¹å‚æ•°å†è¿›è¡Œåˆ‡åˆ†ï¼Œæ¯ä¸ªGPUä¿å­˜1/Nå‚æ•°
    + æ¯å¡16M/n
    + å¤šäº†tpçš„é€šä¿¡

### Zero-Offload
å››ç§è®¡ç®—èŠ‚ç‚¹: FWD,BWD,Param Updateå’Œfloat2hafã€‚

FWDå’ŒBWDæ”¾åœ¨GPUï¼Œåä¸¤ä¸ªæ”¾åœ¨CPUè®¡ç®—

å¤šå¡åœºæ™¯çš„offloadéœ€è¦Stage2ï¼šä¸€ä¸ªCPUè¿›ç¨‹å¯¹åº”ä¸€ä¸ªGPUï¼Œè´Ÿè´£1/Nçš„æ¢¯åº¦å’Œä¼˜åŒ–å™¨çŠ¶æ€ã€‚ä½†æ˜¯GPUå’ŒCPUé€šä¿¡æ€»é‡æ˜¯æ’å®šçš„ï¼Œåªå’Œå‚æ•°é‡æœ‰å…³ï¼Œå’ŒNæ— å…³ã€‚


 
## vLLM
https://mp.weixin.qq.com/s/-5EniAmFf1v9RdxI5-CwiQ





