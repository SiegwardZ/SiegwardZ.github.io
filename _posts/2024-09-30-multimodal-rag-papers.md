---
layout: post
title: Multimodal Retrival-Augmented-Genration Papers
date: 2024-09-30 22:00 +0800
categories: [Research, LLM]
tags: [research, paper, rag]
comments: false
toc: true
---

## Multimodal Fusion
### Learning Transferable Visual Models From Natural Language Supervision
![CLIP](/images/papers/clip.png)

### BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation
![BLIP](/images/papers/blip.png)

### UniIR: Training and Benchmarking Universal Multimodal Information Retrievers
3 types of fusion 

![UniIR](/images/papers/uniir.png){:w="400" h="700"}

## Multimodal RAG
### SnapNTell: Enhancing Entity-Centric Visual Question Answering with Retrieval Augmented Multimodal LLM
![SnapNTell](/images/papers/SnapNTell.png){:w="500" h="900"}
+ Factual VQA tasks demand accurate, concrete answers about real-world entities and phenomena
+ Retrieval-based approach enhance LLMs by integrating external knowledge sources or incorporating relevant information, thereby reduce issues related to long-tail entities and decrease the occurrence of hallucinatory responses.
+ Previous studies have explored retrieval augmentation in text-only settings or image captioning tasks.

### Wiki-LLaVA: Hierarchical Retrieval-Augmented Generation for Multimodal LLMs
![wiki-llava](/images/papers/wiki-llava.png)
+ only use visual information during retrieving

### PreFLMR: Scaling Up Fine-Grained Late-Interaction Multi-modal Retrievers
![PreFLMR](/images/papers/PreFLMR.png)
